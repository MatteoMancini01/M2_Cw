{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper Parameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    }
   ],
   "source": [
    "from src.qwen import load_qwen\n",
    "model_qwen, tokenizer = load_qwen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/riccardo_mancini/M2_Cw/m2_venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    }
   ],
   "source": [
    "from src.set_up_lora import*\n",
    "from src.preprocessor import*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyper Parameters that we want to search for:\n",
    "- $r = (2,4,8)$ \"rank\"\n",
    "- $lr = (10^{-5}, 5 \\times 10^{-5}, 10^{4})$ \"learning rate\"\n",
    "\n",
    "The nested loop below will be very expensive in terms of computation, this will load Qwen2.5 nine times, if your local machine struggles to reload Qwen2.5 that many times, use the alternative code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.qwen import load_qwen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with r=2, lr=1e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps 0:  44%|████▎     | 499/1142 [02:04<02:40,  4.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> final loss: 1.2310\n",
      "Training with r=2, lr=5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps 0:  44%|████▎     | 499/1142 [02:12<02:50,  3.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> final loss: 0.9345\n",
      "Training with r=2, lr=0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps 0:  44%|████▎     | 499/1142 [02:01<02:36,  4.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> final loss: 1.0758\n",
      "Training with r=4, lr=1e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps 0:  44%|████▎     | 499/1142 [02:02<02:38,  4.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> final loss: 1.0886\n",
      "Training with r=4, lr=5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps 0:  44%|████▎     | 499/1142 [02:12<02:51,  3.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> final loss: 0.6507\n",
      "Training with r=4, lr=0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps 0:  44%|████▎     | 499/1142 [02:10<02:48,  3.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> final loss: 0.8168\n",
      "Training with r=8, lr=1e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps 0:  44%|████▎     | 499/1142 [02:12<02:50,  3.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> final loss: 1.1950\n",
      "Training with r=8, lr=5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps 0:  44%|████▎     | 499/1142 [02:10<02:48,  3.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> final loss: 1.0317\n",
      "Training with r=8, lr=0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps 0:  44%|████▎     | 499/1142 [02:09<02:47,  3.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> final loss: 1.0108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "results = []\n",
    "ranks = [2, 4, 8]\n",
    "lrs = [1e-5, 5e-5, 1e-4]\n",
    "\n",
    "for r in ranks:\n",
    "    for lr in lrs:\n",
    "        print(f\"Training with r={r}, lr={lr}\")\n",
    "        model, tokenizer = load_qwen()\n",
    "        trained_model, final_loss = train_lora_model(model, tokenizer, lora_rank=r, learning_rate=lr, train_steps=500)\n",
    "        results.append({\"rank\": r, \"learning_rate\": lr, \"final_loss\": final_loss})\n",
    "        print(f\"-> final loss: {final_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   rank  learning_rate  final_loss\n",
      "0     2        0.00001    1.230973\n",
      "1     2        0.00005    0.934489\n",
      "2     2        0.00010    1.075752\n",
      "3     4        0.00001    1.088630\n",
      "4     4        0.00005    0.650662\n",
      "5     4        0.00010    0.816825\n",
      "6     8        0.00001    1.194976\n",
      "7     8        0.00005    1.031744\n",
      "8     8        0.00010    1.010849\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "HP_search_results_df = pd.DataFrame(results)\n",
    "print(HP_search_results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative (Use only if the code above keeps crashing the kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "from src.qwen import load_qwen\n",
    "from src.set_up_lora import*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,tokenizer = load_qwen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, val_texts, _ = load_and_preprocess(\"data/lotka_volterra_data.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training with r=2, lr=1e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps 0:  44%|████▎     | 499/1142 [02:09<02:47,  3.84it/s]\n",
      "Validating: 100%|██████████| 75/75 [00:09<00:00,  8.27it/s, avg_loss=1.1259]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Train loss: 1.1641\n",
      "-> Validation loss: 1.1259\n",
      "\n",
      "Training with r=2, lr=5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps 0:  44%|████▎     | 499/1142 [02:11<02:49,  3.80it/s]\n",
      "Validating: 100%|██████████| 75/75 [00:07<00:00,  9.42it/s, avg_loss=0.8970]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Train loss: 1.0710\n",
      "-> Validation loss: 0.8970\n",
      "\n",
      "Training with r=2, lr=0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps 0:  44%|████▎     | 499/1142 [02:11<02:49,  3.79it/s]\n",
      "Validating: 100%|██████████| 75/75 [00:09<00:00,  8.12it/s, avg_loss=0.8383]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Train loss: 1.0383\n",
      "-> Validation loss: 0.8383\n",
      "\n",
      "Training with r=4, lr=1e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps 0:  44%|████▎     | 499/1142 [02:05<02:41,  3.97it/s]\n",
      "Validating: 100%|██████████| 75/75 [00:07<00:00, 10.26it/s, avg_loss=1.0203]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Train loss: 1.2273\n",
      "-> Validation loss: 1.0203\n",
      "\n",
      "Training with r=4, lr=5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps 0:  44%|████▎     | 499/1142 [02:06<02:42,  3.96it/s]\n",
      "Validating: 100%|██████████| 75/75 [00:09<00:00,  8.21it/s, avg_loss=0.8529]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Train loss: 0.8702\n",
      "-> Validation loss: 0.8529\n",
      "\n",
      "Training with r=4, lr=0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps 0:  44%|████▎     | 499/1142 [02:00<02:34,  4.15it/s]\n",
      "Validating: 100%|██████████| 75/75 [00:08<00:00,  8.70it/s, avg_loss=0.7795]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Train loss: 0.8465\n",
      "-> Validation loss: 0.7795\n",
      "\n",
      "Training with r=8, lr=1e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps 0:  44%|████▎     | 499/1142 [01:59<02:33,  4.18it/s]\n",
      "Validating: 100%|██████████| 75/75 [00:07<00:00, 10.52it/s, avg_loss=0.9397]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Train loss: 1.0821\n",
      "-> Validation loss: 0.9397\n",
      "\n",
      "Training with r=8, lr=5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps 0:  44%|████▎     | 499/1142 [02:01<02:36,  4.12it/s]\n",
      "Validating: 100%|██████████| 75/75 [00:08<00:00,  8.79it/s, avg_loss=0.8051]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Train loss: 0.9173\n",
      "-> Validation loss: 0.8051\n",
      "\n",
      "Training with r=8, lr=0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps 0:  44%|████▎     | 499/1142 [02:00<02:35,  4.14it/s]\n",
      "Validating: 100%|██████████| 75/75 [00:08<00:00,  8.71it/s, avg_loss=0.7376]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Train loss: 0.7409\n",
      "-> Validation loss: 0.7376\n"
     ]
    }
   ],
   "source": [
    "\n",
    "results = []\n",
    "\n",
    "ranks = [2, 4, 8]\n",
    "lrs = [1e-5, 5e-5, 1e-4]\n",
    "\n",
    "for r in ranks:\n",
    "    for lr in lrs:\n",
    "        print(f\"\\nTraining with r={r}, lr={lr}\")\n",
    "\n",
    "        # Load fresh model\n",
    "        model, _ = load_qwen()\n",
    "        trained_model, final_loss = train_lora_model(model, tokenizer, lora_rank=r, learning_rate=lr, train_steps=500)\n",
    "\n",
    "        val_loss, _ = evaluate_loss_perplexity_val(trained_model, tokenizer, val_texts, 4)\n",
    "\n",
    "        results.append({\"rank\": r, \"learning_rate\": lr, \"Train Loss\": final_loss, \"Validation Loss\": val_loss})\n",
    "        print(f\"-> Train loss: {final_loss:.4f}\")\n",
    "        print(f\"-> Validation loss: {val_loss:.4f}\")\n",
    "\n",
    "        # Clean up to free GPU memory\n",
    "        del model\n",
    "        del trained_model\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   rank  learning_rate  Train Loss  Validation Loss\n",
      "0     2        0.00001    1.164133         1.125851\n",
      "1     2        0.00005    1.071049         0.897050\n",
      "2     2        0.00010    1.038254         0.838324\n",
      "3     4        0.00001    1.227332         1.020328\n",
      "4     4        0.00005    0.870162         0.852907\n",
      "5     4        0.00010    0.846522         0.779493\n",
      "6     8        0.00001    1.082127         0.939741\n",
      "7     8        0.00005    0.917339         0.805061\n",
      "8     8        0.00010    0.740861         0.737563\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "HP_search_results_df = pd.DataFrame(results)\n",
    "print(HP_search_results_df)\n",
    "HP_search_results_df.to_csv(\"hp_tuning_results/hp_tun_rank_lr.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After determining best hyper parameters for \"rank\" and \"learning rate\", we can procede to determine which of the three context lengths $[128, 512, 768]$ perform the best for a maximun of 2000 RLPPP steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training with context_lenghts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps 0:  11%|█▏        | 499/4374 [00:38<04:59, 12.94it/s]\n",
      "Validating: 100%|██████████| 75/75 [00:09<00:00,  8.05it/s, avg_loss=0.7386]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Train loss: 1.0504\n",
      "-> Validation loss: 0.7386\n",
      "\n",
      "Training with context_lenghts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps 0:  44%|████▎     | 499/1142 [02:10<02:48,  3.81it/s]\n",
      "Validating: 100%|██████████| 75/75 [00:09<00:00,  7.57it/s, avg_loss=0.7543]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Train loss: 0.7791\n",
      "-> Validation loss: 0.7543\n",
      "\n",
      "Training with context_lenghts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps 0:  55%|█████▌    | 499/900 [14:26<11:36,  1.74s/it] \n",
      "Validating: 100%|██████████| 75/75 [00:22<00:00,  3.35it/s, avg_loss=0.7788]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Train loss: 0.9044\n",
      "-> Validation loss: 0.7788\n"
     ]
    }
   ],
   "source": [
    "context_lengths = [128, 512, 768]\n",
    "best_r = 8\n",
    "best_lr = 1e-4\n",
    "\n",
    "for cl in context_lengths:\n",
    "    print(f\"\\nTraining with context_lenghts\")\n",
    "\n",
    "    # Load fresh model\n",
    "    model, _ = load_qwen()\n",
    "    trained_model, final_loss = train_lora_model(model, tokenizer, lora_rank=best_r, learning_rate=best_lr, max_ctx_length=cl, train_steps=500)\n",
    "\n",
    "    val_loss, _ = evaluate_loss_perplexity_val(trained_model, tokenizer, val_texts, 4)\n",
    "\n",
    "    results.append({\"context_lengths\": lr, \"Train Loss\": final_loss, \"Validation Loss\": val_loss})\n",
    "    print(f\"-> Train loss: {final_loss:.4f}\")\n",
    "    print(f\"-> Validation loss: {val_loss:.4f}\")\n",
    "\n",
    "    # Clean up to free GPU memory\n",
    "    del model\n",
    "    del trained_model\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    rank  learning_rate  Train Loss  Validation Loss  context_lengths\n",
      "0    2.0        0.00001    1.164133         1.125851              NaN\n",
      "1    2.0        0.00005    1.071049         0.897050              NaN\n",
      "2    2.0        0.00010    1.038254         0.838324              NaN\n",
      "3    4.0        0.00001    1.227332         1.020328              NaN\n",
      "4    4.0        0.00005    0.870162         0.852907              NaN\n",
      "5    4.0        0.00010    0.846522         0.779493              NaN\n",
      "6    8.0        0.00001    1.082127         0.939741              NaN\n",
      "7    8.0        0.00005    0.917339         0.805061              NaN\n",
      "8    8.0        0.00010    0.740861         0.737563              NaN\n",
      "9    NaN            NaN    1.050353         0.738644           0.0001\n",
      "10   NaN            NaN    0.779086         0.754333           0.0001\n",
      "11   NaN            NaN    0.904416         0.778757           0.0001\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "HP_search_results_df = pd.DataFrame(results)\n",
    "print(HP_search_results_df)\n",
    "\n",
    "HP_search_results_df.to_csv(\"hp_tuning_results/hp_tun_cl.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "m2_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
