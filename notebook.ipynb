{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required packages\n",
    "import torch\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open `lotka_volterra_data.h5` file on notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('lotka_volterra_data.h5', 'r') as f:\n",
    "    # Access the full dataset\n",
    "    trajectories = f['trajectories'][:]\n",
    "    time_points = f['time'][:]\n",
    "\n",
    "    # Access a single trajectory\n",
    "    system_id = 0 # First system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checikng shape of the dataset, we expect trajectories to be of size $(1000 \\times 100 \\times 2)$, and time_points of size $(100 \\times 1)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time points shape: (100,)\n",
      "\n",
      "Trajectory shape (pray/predator): (1000, 100, 2)\n"
     ]
    }
   ],
   "source": [
    "print('Time points shape:',time_points.shape)\n",
    "print('')\n",
    "print('Trajectory shape (pray/predator):',trajectories.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_systems, num_time_steps, num_variables = trajectories.shape\n",
    "# Create a DataFrame\n",
    "df_traj = pd.DataFrame({\n",
    "    \"system_id\": np.repeat(np.arange(num_systems), num_time_steps),  # Repeats 0-999, each 100 times\n",
    "    \"time_step\": np.repeat(time_points[np.arange(num_time_steps)], num_systems),    # Cycles 0-99 for each system\n",
    "    \"prey\": trajectories[:, :, 0].flatten(),  # Flatten prey values\n",
    "    \"predator\": trajectories[:, :, 1].flatten()  # Flatten predator values\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>system_id</th>\n",
       "      <th>time_step</th>\n",
       "      <th>prey</th>\n",
       "      <th>predator</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.949917</td>\n",
       "      <td>1.040624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.740551</td>\n",
       "      <td>0.779542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.682246</td>\n",
       "      <td>0.564390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.716674</td>\n",
       "      <td>0.407644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.824511</td>\n",
       "      <td>0.300283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>999</td>\n",
       "      <td>200.0</td>\n",
       "      <td>0.901549</td>\n",
       "      <td>0.579420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>999</td>\n",
       "      <td>200.0</td>\n",
       "      <td>0.957527</td>\n",
       "      <td>0.539055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>999</td>\n",
       "      <td>200.0</td>\n",
       "      <td>1.036460</td>\n",
       "      <td>0.515615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>999</td>\n",
       "      <td>200.0</td>\n",
       "      <td>1.129212</td>\n",
       "      <td>0.510619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>999</td>\n",
       "      <td>200.0</td>\n",
       "      <td>1.223701</td>\n",
       "      <td>0.524988</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       system_id  time_step      prey  predator\n",
       "0              0        0.0  0.949917  1.040624\n",
       "1              0        0.0  0.740551  0.779542\n",
       "2              0        0.0  0.682246  0.564390\n",
       "3              0        0.0  0.716674  0.407644\n",
       "4              0        0.0  0.824511  0.300283\n",
       "...          ...        ...       ...       ...\n",
       "99995        999      200.0  0.901549  0.579420\n",
       "99996        999      200.0  0.957527  0.539055\n",
       "99997        999      200.0  1.036460  0.515615\n",
       "99998        999      200.0  1.129212  0.510619\n",
       "99999        999      200.0  1.223701  0.524988\n",
       "\n",
       "[100000 rows x 4 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_traj # Visualising data in dataframe format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.080808080808081\n"
     ]
    }
   ],
   "source": [
    "time_step = df_traj['time_step'].to_numpy()\n",
    "\n",
    "print(time_step[4925])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 (a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grouping prey and predator into arrays to determine the maximum value for scaling procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "prey_array = df_traj['prey'].to_numpy() # Converting to numpy array\n",
    "predator_array = df_traj['predator'].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.10 Scaling Dataset `lotka_volterra_data.h5`\n",
    "\n",
    "As we will see in the `Table` presented below, in the original dataset we have laues that vary significantly. To standardize the numeric range, we are going to use [quantiles]( https://en.wikipedia.org/wiki/Quantile). A quantile is a value that divides a dataset into equal-sized intervals, indicating the data points below which a given percentage if observations fall. From the project instructions it is adviced to apply a simple scaling:\n",
    "$$\n",
    "x_t' = \\frac{x_t}{\\alpha}\n",
    "$$\n",
    "where $\\alpha$ should be chosen based on the distribution of the dataset `lotka_volterra_data.h5`.\n",
    "\n",
    "In our particular case we want most of our dataset to be in range $[0,10]$. This is coded in the [`preprocessor.py`](https://github.com/MatteoMancini01/M2_Cw/blob/main/src/preprocessor.py) file, which appropriate docstrings.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `numpy.quantile()`\n",
    "\n",
    "For scaling our dataset we want to use [`numpy.quantile()`](https://numpy.org/doc/2.1/reference/generated/numpy.quantile.html). The `numpy.quantile()` function calculates the quantiles of a given NumPy array. Quantiles are cut points that devide the data into intercals with equal probability. Thus `numpy.quantile()`can be used to scale our dataset dynamically, without having to worry about choosing the appropriate value for $\\alpha$.\n",
    "\n",
    "All of this is implemented in the function `scaling_operator`, which also <b>rounds</b> each datapoint using `numpy.round()`, this is set to 3 decimal places."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import class Preprocessor from src/preprocessor.py\n",
    "from src.preprocessor import Preprocessor\n",
    "\n",
    "# Set scaling_operator to function \n",
    "scaling_operator = Preprocessor.scaling_operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaling factor: 0.25283724\n"
     ]
    }
   ],
   "source": [
    "trajectories_scaled, scaling_factor = scaling_operator(trajectories, 0.9, 10)\n",
    "print('Scaling factor:', scaling_factor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collecting scaled data into `pandas.DataFrame` format, in particular, we want to construct a $100000\\times 4$ table, (number of rows $= 1000 \\times 100$). With four colums, of which three are `time_step`, `prey` and `predator`, but with an additional one `system_id` (this separates the $1000$ different systems), which will be later used to convert our timeseries data into string format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_systems_scaled, num_time_steps_scaled, num_variables_scaled = trajectories_scaled.shape\n",
    "\n",
    "# Create a DataFrame\n",
    "df_traj_scaled = pd.DataFrame({\n",
    "    \"system_id\": np.repeat(np.arange(num_systems_scaled), num_time_steps_scaled),  # Repeats 0-999, each 100 times\n",
    "    \"time_step\": np.repeat(time_points[np.arange(num_time_steps_scaled)], num_systems_scaled),  # Cycles 0-200 (array.shape = (100,)) for each system\n",
    "    \"prey\": trajectories_scaled[:, :, 0].flatten(),  # Flatten prey values\n",
    "    \"predator\": trajectories_scaled[:, :, 1].flatten()  # Flatten predator values\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualising `df_traj_scaled`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>system_id</th>\n",
       "      <th>time_step</th>\n",
       "      <th>prey</th>\n",
       "      <th>predator</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.757</td>\n",
       "      <td>4.116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.929</td>\n",
       "      <td>3.083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.698</td>\n",
       "      <td>2.232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.835</td>\n",
       "      <td>1.612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.261</td>\n",
       "      <td>1.188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>999</td>\n",
       "      <td>200.0</td>\n",
       "      <td>3.566</td>\n",
       "      <td>2.292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>999</td>\n",
       "      <td>200.0</td>\n",
       "      <td>3.787</td>\n",
       "      <td>2.132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>999</td>\n",
       "      <td>200.0</td>\n",
       "      <td>4.099</td>\n",
       "      <td>2.039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>999</td>\n",
       "      <td>200.0</td>\n",
       "      <td>4.466</td>\n",
       "      <td>2.020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>999</td>\n",
       "      <td>200.0</td>\n",
       "      <td>4.840</td>\n",
       "      <td>2.076</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       system_id  time_step   prey  predator\n",
       "0              0        0.0  3.757     4.116\n",
       "1              0        0.0  2.929     3.083\n",
       "2              0        0.0  2.698     2.232\n",
       "3              0        0.0  2.835     1.612\n",
       "4              0        0.0  3.261     1.188\n",
       "...          ...        ...    ...       ...\n",
       "99995        999      200.0  3.566     2.292\n",
       "99996        999      200.0  3.787     2.132\n",
       "99997        999      200.0  4.099     2.039\n",
       "99998        999      200.0  4.466     2.020\n",
       "99999        999      200.0  4.840     2.076\n",
       "\n",
       "[100000 rows x 4 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_traj_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting `prey` and `predator` columns into array using [`pandas.DataFrame.to_numpy`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_numpy.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "prey_array_scaled = df_traj_scaled['prey'].to_numpy() # Converting to numpy array\n",
    "predator_array_scaled = df_traj_scaled['predator'].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining a function that calculates the percentage of values in an array that fall outside a given range. (This seem tedious, as we set a value for quantile in the function `scaling_operator`, e.g. $q = 0.9$, means that only $10%$ of the values will be out of our custom range. But this will be used to measure what percentage of datapoints in the original dataset is outside a specific range.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaling_measure(arr, min_val, max_val):\n",
    "    \n",
    "    \"\"\"\n",
    "    Calculates the percentage of values in an array that fall outside a given range.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    arr : array-like\n",
    "        The input numerical data.\n",
    "    min_val : float\n",
    "        The minimum acceptable value.\n",
    "    max_val : float\n",
    "        The maximum acceptable value.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    str\n",
    "        The percentage of values outside the range, formatted as a string.\n",
    "    \"\"\"\n",
    "\n",
    "    # Count values about the max range\n",
    "    outside_count = np.sum((arr < min_val)|(arr > max_val))\n",
    "\n",
    "    # Calculating the pergentage of values outside max range\n",
    "    percentage_outside = (outside_count/arr.size)*100\n",
    "\n",
    "    return f'{percentage_outside:.2f}%'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collecting scaling information into a Table using `pandas.DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_val = 0\n",
    "max_val = 1\n",
    "Table_1 = pd.DataFrame({\n",
    "\n",
    "    'Pray': [max(prey_array), np.mean(prey_array), min(prey_array), scaling_measure(prey_array, min_val, max_val)],\n",
    "    'Pray after scaling': [max(prey_array_scaled), np.mean(prey_array_scaled), min(prey_array_scaled), scaling_measure(prey_array_scaled, min_val, max_val)],\n",
    "    'Predator': [max(predator_array), np.mean(predator_array), min(predator_array), scaling_measure(predator_array, min_val, max_val)],\n",
    "    'Predator after scaling': [max(predator_array_scaled), np.mean(predator_array_scaled), min(predator_array_scaled), scaling_measure(predator_array_scaled, min_val, max_val)],\n",
    "    \n",
    "})\n",
    "Table_1.index = [\"Maximim Value\", \"Mean Value\", \"Minimum Value\", f\"Values outside the range {min_val}-{max_val}\"] # Adding index for each row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " From the table below, we can observe, scaling was successful. The reason why we want to test how many data points are outside the range $[0,1]$, is due to the fact that a lot of data points in the original dataset (pre-scaling) are very small, many of order $10^{-3}$ (and smaller order $10^{-4}$), which may affect the tokenisation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pray</th>\n",
       "      <th>Pray after scaling</th>\n",
       "      <th>Predator</th>\n",
       "      <th>Predator after scaling</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Maximim Value</th>\n",
       "      <td>13.740113</td>\n",
       "      <td>54.344002</td>\n",
       "      <td>4.76849</td>\n",
       "      <td>18.860001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mean Value</th>\n",
       "      <td>1.698114</td>\n",
       "      <td>6.71623</td>\n",
       "      <td>0.569606</td>\n",
       "      <td>2.252858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Minimum Value</th>\n",
       "      <td>0.002077</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Values outside the range 0-1</th>\n",
       "      <td>63.11%</td>\n",
       "      <td>93.82%</td>\n",
       "      <td>12.21%</td>\n",
       "      <td>77.31%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   Pray Pray after scaling  Predator  \\\n",
       "Maximim Value                 13.740113          54.344002   4.76849   \n",
       "Mean Value                     1.698114            6.71623  0.569606   \n",
       "Minimum Value                  0.002077              0.008  0.000037   \n",
       "Values outside the range 0-1     63.11%             93.82%    12.21%   \n",
       "\n",
       "                             Predator after scaling  \n",
       "Maximim Value                             18.860001  \n",
       "Mean Value                                 2.252858  \n",
       "Minimum Value                                   0.0  \n",
       "Values outside the range 0-1                 77.31%  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Table_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the last row, we can see that we have a major improvement for both `prey` and `predator` categories, the percentage of values outside the range $[0,1]$ has increased in `prey` by ~ $30\\%$ and `predator` by ~ $65\\%$. Thus, scaling was successful. Now we can proceed with the next step, i.e. converting the scaled dataset to strings, for compatibility with [Qwen2.5]( https://github.com/QwenLM/Qwen2.5)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.11 Loading Qwen2.5\n",
    "\n",
    "Below a short demonstration on how to use `load_qwen()` from `src.qwen`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.qwen import load_qwen # Import load_qwen\n",
    "model, tokenizer = load_qwen() # set model = model and tokeinzer = tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying with examples provided at the end of project instructions, see [LLMTIME Preprocessing Scheme](https://github.com/MatteoMancini01/M2_Cw/blob/main/instructions/main.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16, 13, 17, 18]\n",
      "\n",
      "[16, 659, 220, 17, 220, 18]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer(\"1.23\", return_tensors=\"pt\")[\"input_ids\"].tolist()[0])\n",
    "print('')\n",
    "print(tokenizer(\"1 . 2 3\", return_tensors=\"pt\")[\"input_ids\"].tolist()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying to tokenise $[0.25,1.50;0.27,1.47;0.31,1.42]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15, 13, 17, 20, 11, 16, 13, 20, 15, 26, 15, 13, 17, 22, 11, 16, 13, 19, 22, 26, 15, 13, 18, 16, 11, 16, 13, 19, 17]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer(\"0.25,1.50;0.27,1.47;0.31,1.42\", return_tensors='pt')[\"input_ids\"].tolist()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of how `load_qwen()` works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    }
   ],
   "source": [
    "text = 'Hello, world' # Define input text\n",
    "input_ids = tokenizer(text, return_tensors='pt').input_ids # Tokenize text \n",
    "output = model.generate(input_ids, max_length = 50) # Generate output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above code line 3 `output`, the variable `max_length` determines how many more words will the model predict when inputing text, e.g. `text = Hello, world`, as we can see from the below output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 9707,    11,  1879,  2219,    40,  2776,   264, 48948,   304,  5994,\n",
       "           323,   358,  1184,  1492,   448,   847,  2390,    13,   358,   614,\n",
       "           279,  2701,  2038,  1447, 73594,  3346,   198,  1722,  1887,   271,\n",
       "           474,  2399,   262,   330, 12501,   698,   692,  2830,  1887,   368,\n",
       "           341,   262,  8879, 12419,   445,  9707,    11,  1879, 22988,   532]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above tensor we recognise that the first 3 tokens are related to our text, the rest of the tokens is predicted text determined from the model, as we will see below when decoding `output`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, world!\n",
      "\n",
      "I'm a beginner in Go and I need help with my project. I have the following code:\n",
      "\n",
      "```go\n",
      "package main\n",
      "\n",
      "import (\n",
      "    \"fmt\"\n",
      ")\n",
      "\n",
      "func main() {\n",
      "    fmt.Println(\"Hello, world!\")\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(output[0], skip_special_tokens=True)) # Decoding output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.12 Converting Scaled Dataset into Strings\n",
    "\n",
    "We now have seen how tokenisation for text works! There is a small issue, Qwen2.5 is only designed to convert text, i.e. strings in Python, to tokens, while our dataset is a timeseries composed of 2 variables prey and predator over a time series of size 100, this is repated for a 1000 samples. Thus, before we proceed with tokenisation, we require to convert the time series data into sets of strings. To do so we are going to define a fucntion `array_to_string(data)`, and a function to convert string to array `sring_to_array(formatted_string)` (both functions are in [preprocessor.py](https://github.com/MatteoMancini01/M2_Cw/blob/main/src/preprocessor.py))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "âš  Note: The function `array_to_string` is specifically designed for the dataset `lotka_volterra_data.h5`, in particular after converting `trajectories` into a `panda.DataFrame` format, with columns `system_id` (labeling each system from 0 to 999), columns `prey` and `predator`, each  displaying 100 data points for every `system_id`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.preprocessor import Preprocessor\n",
    "\n",
    "array_to_string = Preprocessor.array_to_string # Importing array_to_string(data) to convert timeseries to string\n",
    "string_to_array = Preprocessor.string_to_array # Importing string_to_array(formatted_string) to convert strings back to arrays\n",
    "\n",
    "traject_scaled_string = array_to_string(df_traj_scaled) # Converting df_traj_scaled into string format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking result post-conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system_id\n",
      "0      3.757,4.116;2.929,3.083;2.698,2.232;2.835,1.61...\n",
      "1      3.842,3.977;4.266,3.25;4.987,2.715;6.019,2.346...\n",
      "2      4.245,4.401;3.378,3.56;3.038,2.801;3.04,2.187;...\n",
      "3      4.115,4.567;2.628,4.406;1.746,3.903;1.283,3.28...\n",
      "4      3.276,3.167;3.533,2.247;4.089,1.611;4.943,1.17...\n",
      "                             ...                        \n",
      "995    3.918,4.625;2.139,3.254;1.521,2.104;1.327,1.32...\n",
      "996    3.595,4.653;2.249,3.797;1.681,2.896;1.484,2.15...\n",
      "997    4.465,4.433;4.055,4.03;3.9,3.615;3.981,3.237;4...\n",
      "998    4.476,4.017;3.167,3.167;2.564,2.341;2.362,1.68...\n",
      "999    4.035,4.489;3.028,4.1;2.481,3.453;2.277,2.794;...\n",
      "Length: 1000, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(traject_scaled_string) # Print output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also want to test the function `string_to_array`, this is done below for the first `system_id` string data format, i.e. `system_id` $ = 0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3.757  4.116]\n",
      " [ 2.929  3.083]\n",
      " [ 2.698  2.232]\n",
      " [ 2.835  1.612]\n",
      " [ 3.261  1.188]\n",
      " [ 3.973  0.909]\n",
      " [ 4.995  0.736]\n",
      " [ 6.35   0.642]\n",
      " [ 8.033  0.611]\n",
      " [ 9.978  0.64 ]\n",
      " [12.027  0.741]\n",
      " [13.915  0.938]\n",
      " [15.283  1.271]\n",
      " [15.732  1.785]\n",
      " [14.933  2.492]\n",
      " [12.876  3.282]\n",
      " [10.089  3.882]\n",
      " [ 7.451  4.015]\n",
      " [ 5.574  3.666]\n",
      " [ 4.52   3.066]\n",
      " [ 4.08   2.448]\n",
      " [ 4.068  1.929]\n",
      " [ 4.385  1.536]\n",
      " [ 4.991  1.259]\n",
      " [ 5.869  1.081]\n",
      " [ 7.002  0.986]\n",
      " [ 8.344  0.963]\n",
      " [ 9.801  1.013]\n",
      " [11.212  1.145]\n",
      " [12.356  1.373]\n",
      " [12.979  1.715]\n",
      " [12.864  2.165]\n",
      " [11.941  2.672]\n",
      " [10.379  3.118]\n",
      " [ 8.59   3.351]\n",
      " [ 7.014  3.296]\n",
      " [ 5.897  3.004]\n",
      " [ 5.265  2.6  ]\n",
      " [ 5.044  2.193]\n",
      " [ 5.154  1.845]\n",
      " [ 5.539  1.577]\n",
      " [ 6.165  1.389]\n",
      " [ 6.997  1.277]\n",
      " [ 7.985  1.235]\n",
      " [ 9.047  1.263]\n",
      " [10.065  1.363]\n",
      " [10.892  1.539]\n",
      " [11.366  1.792]\n",
      " [11.36   2.108]\n",
      " [10.829  2.451]\n",
      " [ 9.87   2.75 ]\n",
      " [ 8.698  2.927]\n",
      " [ 7.569  2.934]\n",
      " [ 6.673  2.781]\n",
      " [ 6.095  2.527]\n",
      " [ 5.834  2.238]\n",
      " [ 5.856  1.969]\n",
      " [ 6.119  1.748]\n",
      " [ 6.586  1.585]\n",
      " [ 7.217  1.485]\n",
      " [ 7.96   1.445]\n",
      " [ 8.747  1.465]\n",
      " [ 9.484  1.546]\n",
      " [10.065  1.686]\n",
      " [10.389  1.879]\n",
      " [10.378  2.11 ]\n",
      " [10.015  2.349]\n",
      " [ 9.366  2.551]\n",
      " [ 8.563  2.672]\n",
      " [ 7.76   2.684]\n",
      " [ 7.09   2.589]\n",
      " [ 6.629  2.419]\n",
      " [ 6.4    2.215]\n",
      " [ 6.395  2.013]\n",
      " [ 6.589  1.84 ]\n",
      " [ 6.95   1.709]\n",
      " [ 7.44   1.626]\n",
      " [ 8.01   1.593]\n",
      " [ 8.602  1.611]\n",
      " [ 9.141  1.678]\n",
      " [ 9.552  1.791]\n",
      " [ 9.765  1.942]\n",
      " [ 9.735  2.115]\n",
      " [ 9.462  2.286]\n",
      " [ 8.994  2.426]\n",
      " [ 8.418  2.507]\n",
      " [ 7.838  2.513]\n",
      " [ 7.343  2.447]\n",
      " [ 6.995  2.326]\n",
      " [ 6.817  2.178]\n",
      " [ 6.812  2.029]\n",
      " [ 6.963  1.897]\n",
      " [ 7.244  1.795]\n",
      " [ 7.624  1.731]\n",
      " [ 8.061  1.707]\n",
      " [ 8.504  1.724]\n",
      " [ 8.898  1.78 ]\n",
      " [ 9.187  1.871]\n",
      " [ 9.324  1.988]\n",
      " [ 9.283  2.117]]\n"
     ]
    }
   ],
   "source": [
    "print(string_to_array(traject_scaled_string[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can observe from the above output, we successfully converted string back to array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.13 Tokenisation \n",
    "\n",
    "We provided few basic examples on how to use `load_qwen()` in section 2.11, with some text and numbers (string form). We now want to proceed and tokenise our data, to achieve this, we designed a function for our particular needs that uses `model, tokenizer = load_qwen()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.qwen import tokenize_time_series\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenised_data = tokenize_time_series(traject_scaled_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualising data before tokenisation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "system_id\n",
       "0      3.757,4.116;2.929,3.083;2.698,2.232;2.835,1.61...\n",
       "1      3.842,3.977;4.266,3.25;4.987,2.715;6.019,2.346...\n",
       "2      4.245,4.401;3.378,3.56;3.038,2.801;3.04,2.187;...\n",
       "3      4.115,4.567;2.628,4.406;1.746,3.903;1.283,3.28...\n",
       "4      3.276,3.167;3.533,2.247;4.089,1.611;4.943,1.17...\n",
       "                             ...                        \n",
       "995    3.918,4.625;2.139,3.254;1.521,2.104;1.327,1.32...\n",
       "996    3.595,4.653;2.249,3.797;1.681,2.896;1.484,2.15...\n",
       "997    4.465,4.433;4.055,4.03;3.9,3.615;3.981,3.237;4...\n",
       "998    4.476,4.017;3.167,3.167;2.564,2.341;2.362,1.68...\n",
       "999    4.035,4.489;3.028,4.1;2.481,3.453;2.277,2.794;...\n",
       "Length: 1000, dtype: object"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traject_scaled_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After tokenisation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "system_id\n",
       "0      [input_ids, attention_mask]\n",
       "1      [input_ids, attention_mask]\n",
       "2      [input_ids, attention_mask]\n",
       "3      [input_ids, attention_mask]\n",
       "4      [input_ids, attention_mask]\n",
       "                  ...             \n",
       "995    [input_ids, attention_mask]\n",
       "996    [input_ids, attention_mask]\n",
       "997    [input_ids, attention_mask]\n",
       "998    [input_ids, attention_mask]\n",
       "999    [input_ids, attention_mask]\n",
       "Length: 1000, dtype: object"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenised_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clsoer look at two examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Two examples of tokens from tokenised_data:\n",
      "\n",
      "Preprocessed data:\n",
      "4.115,4.567;2.628,4.406;1.746,3.903;1.283,3.281;1.064,2.682;0.985,2.167;1.001,1.747;1.097,1.415;1.276,1.159;1.55,0.967;1.943,0.829;2.482,0.737;3.2,0.687;4.123,0.678;5.257,0.716;6.562,0.815;7.924,0.998;9.13,1.307;9.86,1.792;9.749,2.494;8.564,3.374;6.523,4.207;4.348,4.651;2.719,4.538;1.766,4.031;1.271,3.387;1.035,2.764;0.945,2.226;0.951,1.787;1.036,1.44;1.201,1.173;1.456,0.973;1.825,0.827;2.337,0.728;3.024,0.67;3.917,0.652;5.029,0.68;6.335,0.763;7.739,0.925;9.045,1.203;9.947,1.649;10.062,2.316;9.091,3.192;7.12,4.099;4.814,4.678;2.985,4.677;1.89,4.211;1.314,3.563;1.034,2.913;0.918,2.344;0.904,1.876;0.969,1.505;1.11,1.218;1.338,1.001;1.671,0.841;2.137,0.73;2.772,0.661;3.609,0.631;4.669,0.644;5.946,0.706;7.369,0.838;8.777,1.073;9.892,1.46;10.333,2.058;9.727,2.895;7.978,3.859;5.6,4.616;3.489,4.82;2.134,4.464;1.414,3.824;1.061,3.142;0.904,2.528;0.861,2.019;0.9,1.612;1.013,1.295;1.205,1.054;1.493,0.875;1.904,0.747;2.469,0.663;3.226,0.618;4.206,0.612;5.418,0.651;6.824,0.75;8.304,0.935;9.625,1.25;10.438,1.755;10.32,2.508;8.998,3.473;6.709,4.402;4.307,4.883;2.589,4.729;1.63,4.156;1.149,3.455;0.924,2.793;0.84,2.229;0.846,1.774;0.925,1.417;1.078,1.143;1.317,0.937;1.665,0.787\n",
      "\n",
      "After tokenisation:\n",
      "[19, 13, 16, 16, 20, 11, 19, 13, 20, 21, 22, 26, 17, 13, 21, 17, 23, 11, 19, 13, 19, 15, 21, 26, 16, 13, 22, 19, 21, 11, 18, 13, 24, 15, 18, 26, 16, 13, 17, 23, 18, 11, 18, 13, 17, 23, 16, 26, 16, 13, 15, 21, 19, 11, 17, 13, 21, 23, 17, 26, 15, 13, 24, 23, 20, 11, 17, 13, 16, 21, 22, 26, 16, 13, 15, 15, 16, 11, 16, 13, 22, 19, 22, 26, 16, 13, 15, 24, 22, 11, 16, 13, 19, 16, 20, 26, 16, 13, 17, 22, 21, 11, 16, 13, 16, 20, 24, 26, 16, 13, 20, 20, 11, 15, 13, 24, 21, 22, 26, 16, 13, 24, 19, 18, 11, 15, 13, 23, 17, 24, 26, 17, 13, 19, 23, 17, 11, 15, 13, 22, 18, 22, 26, 18, 13, 17, 11, 15, 13, 21, 23, 22, 26, 19, 13, 16, 17, 18, 11, 15, 13, 21, 22, 23, 26, 20, 13, 17, 20, 22, 11, 15, 13, 22, 16, 21, 26, 21, 13, 20, 21, 17, 11, 15, 13, 23, 16, 20, 26, 22, 13, 24, 17, 19, 11, 15, 13, 24, 24, 23, 26, 24, 13, 16, 18, 11, 16, 13, 18, 15, 22, 26, 24, 13, 23, 21, 11, 16, 13, 22, 24, 17, 26, 24, 13, 22, 19, 24, 11, 17, 13, 19, 24, 19, 26, 23, 13, 20, 21, 19, 11, 18, 13, 18, 22, 19, 26, 21, 13, 20, 17, 18, 11, 19, 13, 17, 15, 22, 26, 19, 13, 18, 19, 23, 11, 19, 13, 21, 20, 16, 26, 17, 13, 22, 16, 24, 11, 19, 13, 20, 18, 23, 26, 16, 13, 22, 21, 21, 11, 19, 13, 15, 18, 16, 26, 16, 13, 17, 22, 16, 11, 18, 13, 18, 23, 22, 26, 16, 13, 15, 18, 20, 11, 17, 13, 22, 21, 19, 26, 15, 13, 24, 19, 20, 11, 17, 13, 17, 17, 21, 26, 15, 13, 24, 20, 16, 11, 16, 13, 22, 23, 22, 26, 16, 13, 15, 18, 21, 11, 16, 13, 19, 19, 26, 16, 13, 17, 15, 16, 11, 16, 13, 16, 22, 18, 26, 16, 13, 19, 20, 21, 11, 15, 13, 24, 22, 18, 26, 16, 13, 23, 17, 20, 11, 15, 13, 23, 17, 22, 26, 17, 13, 18, 18, 22, 11, 15, 13, 22, 17, 23, 26, 18, 13, 15, 17, 19, 11, 15, 13, 21, 22, 26, 18, 13, 24, 16, 22, 11, 15, 13, 21, 20, 17, 26, 20, 13, 15, 17, 24, 11, 15, 13, 21, 23, 26, 21, 13, 18, 18, 20, 11, 15, 13, 22, 21, 18, 26, 22, 13, 22, 18, 24, 11, 15, 13, 24, 17, 20, 26, 24, 13, 15, 19, 20, 11, 16, 13, 17, 15, 18, 26, 24, 13, 24, 19, 22, 11, 16, 13, 21, 19, 24, 26, 16, 15, 13, 15, 21, 17, 11, 17, 13, 18, 16, 21, 26, 24, 13, 15, 24, 16, 11, 18, 13, 16, 24, 17, 26, 22, 13, 16, 17, 11, 19, 13, 15, 24, 24, 26, 19, 13, 23, 16, 19, 11, 19, 13, 21, 22, 23, 26, 17, 13, 24, 23, 20, 11, 19, 13, 21, 22, 22, 26, 16, 13, 23, 24, 11, 19, 13, 17, 16, 16, 26, 16, 13, 18, 16, 19, 11, 18, 13, 20, 21, 18, 26, 16, 13, 15, 18, 19, 11, 17, 13, 24, 16, 18, 26, 15, 13, 24, 16, 23, 11, 17, 13, 18, 19, 19, 26, 15, 13, 24, 15, 19, 11, 16, 13, 23, 22, 21, 26, 15, 13, 24, 21, 24, 11, 16, 13, 20, 15, 20, 26, 16, 13, 16, 16, 11, 16, 13, 17, 16, 23, 26, 16, 13, 18, 18, 23, 11, 16, 13, 15, 15, 16, 26, 16, 13, 21, 22, 16, 11, 15, 13, 23, 19, 16, 26, 17, 13, 16, 18, 22, 11, 15, 13, 22, 18, 26, 17, 13, 22, 22, 17, 11, 15, 13, 21, 21, 16, 26, 18, 13, 21, 15, 24, 11, 15, 13, 21, 18, 16, 26, 19, 13, 21, 21, 24, 11, 15, 13, 21, 19, 19, 26, 20, 13, 24, 19, 21, 11, 15, 13, 22, 15, 21, 26, 22, 13, 18, 21, 24, 11, 15, 13, 23, 18, 23, 26, 23, 13, 22, 22, 22, 11, 16, 13, 15, 22, 18, 26, 24, 13, 23, 24, 17, 11, 16, 13, 19, 21, 26, 16, 15, 13, 18, 18, 18, 11, 17, 13, 15, 20, 23, 26, 24, 13, 22, 17, 22, 11, 17, 13, 23, 24, 20, 26, 22, 13, 24, 22, 23, 11, 18, 13, 23, 20, 24, 26, 20, 13, 21, 11, 19, 13, 21, 16, 21, 26, 18, 13, 19, 23, 24, 11, 19, 13, 23, 17, 26, 17, 13, 16, 18, 19, 11, 19, 13, 19, 21, 19, 26, 16, 13, 19, 16, 19, 11, 18, 13, 23, 17, 19, 26, 16, 13, 15, 21, 16, 11, 18, 13, 16, 19, 17, 26, 15, 13, 24, 15, 19, 11, 17, 13, 20, 17, 23, 26, 15, 13, 23, 21, 16, 11, 17, 13, 15, 16, 24, 26, 15, 13, 24, 11, 16, 13, 21, 16, 17, 26, 16, 13, 15, 16, 18, 11, 16, 13, 17, 24, 20, 26, 16, 13, 17, 15, 20, 11, 16, 13, 15, 20, 19, 26, 16, 13, 19, 24, 18, 11, 15, 13, 23, 22, 20, 26, 16, 13, 24, 15, 19, 11, 15, 13, 22, 19, 22, 26, 17, 13, 19, 21, 24, 11, 15, 13, 21, 21, 18, 26, 18, 13, 17, 17, 21, 11, 15, 13, 21, 16, 23, 26, 19, 13, 17, 15, 21, 11, 15, 13, 21, 16, 17, 26, 20, 13, 19, 16, 23, 11, 15, 13, 21, 20, 16, 26, 21, 13, 23, 17, 19, 11, 15, 13, 22, 20, 26, 23, 13, 18, 15, 19, 11, 15, 13, 24, 18, 20, 26, 24, 13, 21, 17, 20, 11, 16, 13, 17, 20, 26, 16, 15, 13, 19, 18, 23, 11, 16, 13, 22, 20, 20, 26, 16, 15, 13, 18, 17, 11, 17, 13, 20, 15, 23, 26, 23, 13, 24, 24, 23, 11, 18, 13, 19, 22, 18, 26, 21, 13, 22, 15, 24, 11, 19, 13, 19, 15, 17, 26, 19, 13, 18, 15, 22, 11, 19, 13, 23, 23, 18, 26, 17, 13, 20, 23, 24, 11, 19, 13, 22, 17, 24, 26, 16, 13, 21, 18, 11, 19, 13, 16, 20, 21, 26, 16, 13, 16, 19, 24, 11, 18, 13, 19, 20, 20, 26, 15, 13, 24, 17, 19, 11, 17, 13, 22, 24, 18, 26, 15, 13, 23, 19, 11, 17, 13, 17, 17, 24, 26, 15, 13, 23, 19, 21, 11, 16, 13, 22, 22, 19, 26, 15, 13, 24, 17, 20, 11, 16, 13, 19, 16, 22, 26, 16, 13, 15, 22, 23, 11, 16, 13, 16, 19, 18, 26, 16, 13, 18, 16, 22, 11, 15, 13, 24, 18, 22, 26, 16, 13, 21, 21, 20, 11, 15, 13, 22, 23, 22]\n",
      "Length of the above token: 1180\n",
      "\n",
      "\n",
      "Preprocessed data:\n",
      "4.738,4.035;3.557,3.817;2.752,3.472;2.234,3.067;1.901,2.664;1.698,2.287;1.586,1.95;1.539,1.657;1.545,1.407;1.596,1.196;1.687,1.02;1.817,0.875;1.988,0.755;2.201,0.657;2.458,0.577;2.764,0.514;3.12,0.463;3.53,0.425;3.995,0.396;4.516,0.377;5.09,0.366;5.713,0.364;6.376,0.37;7.068,0.385;7.771,0.411;8.466,0.449;9.128,0.501;9.73,0.57;10.243,0.66;10.634,0.775;10.872,0.917;10.93,1.091;10.787,1.296;10.434,1.529;9.878,1.779;9.149,2.029;8.295,2.259;7.384,2.445;6.484,2.566;5.65,2.615;4.929,2.588;4.336,2.497;3.871,2.361;3.522,2.195;3.274,2.017;3.111,1.838;3.021,1.666;2.992,1.506;3.018,1.362;3.093,1.234;3.213,1.123;3.376,1.027;3.579,0.947;3.822,0.882;4.101,0.829;4.415,0.789;4.761,0.76;5.134,0.742;5.528,0.736;5.935,0.74;6.346,0.755;6.75,0.782;7.134,0.821;7.482,0.873;7.782,0.938;8.016,1.016;8.173,1.108;8.241,1.213;8.212,1.329;8.086,1.452;7.865,1.578;7.561,1.7;7.19,1.812;6.775,1.905;6.338,1.974;5.904,2.014;5.493,2.023;5.121,2.004;4.801,1.958;4.535,1.892;4.326,1.811;4.171,1.721;4.068,1.628;4.012,1.535;3.999,1.445;4.026,1.362;4.09,1.285;4.187,1.217;4.316,1.158;4.473,1.107;4.654,1.066;4.858,1.034;5.079,1.011;5.314,0.997;5.556,0.992;5.8,0.996;6.04,1.008;6.268,1.03;6.477,1.06;6.659,1.098\n",
      "\n",
      "After tokenisation:\n",
      "[19, 13, 22, 18, 23, 11, 19, 13, 15, 18, 20, 26, 18, 13, 20, 20, 22, 11, 18, 13, 23, 16, 22, 26, 17, 13, 22, 20, 17, 11, 18, 13, 19, 22, 17, 26, 17, 13, 17, 18, 19, 11, 18, 13, 15, 21, 22, 26, 16, 13, 24, 15, 16, 11, 17, 13, 21, 21, 19, 26, 16, 13, 21, 24, 23, 11, 17, 13, 17, 23, 22, 26, 16, 13, 20, 23, 21, 11, 16, 13, 24, 20, 26, 16, 13, 20, 18, 24, 11, 16, 13, 21, 20, 22, 26, 16, 13, 20, 19, 20, 11, 16, 13, 19, 15, 22, 26, 16, 13, 20, 24, 21, 11, 16, 13, 16, 24, 21, 26, 16, 13, 21, 23, 22, 11, 16, 13, 15, 17, 26, 16, 13, 23, 16, 22, 11, 15, 13, 23, 22, 20, 26, 16, 13, 24, 23, 23, 11, 15, 13, 22, 20, 20, 26, 17, 13, 17, 15, 16, 11, 15, 13, 21, 20, 22, 26, 17, 13, 19, 20, 23, 11, 15, 13, 20, 22, 22, 26, 17, 13, 22, 21, 19, 11, 15, 13, 20, 16, 19, 26, 18, 13, 16, 17, 11, 15, 13, 19, 21, 18, 26, 18, 13, 20, 18, 11, 15, 13, 19, 17, 20, 26, 18, 13, 24, 24, 20, 11, 15, 13, 18, 24, 21, 26, 19, 13, 20, 16, 21, 11, 15, 13, 18, 22, 22, 26, 20, 13, 15, 24, 11, 15, 13, 18, 21, 21, 26, 20, 13, 22, 16, 18, 11, 15, 13, 18, 21, 19, 26, 21, 13, 18, 22, 21, 11, 15, 13, 18, 22, 26, 22, 13, 15, 21, 23, 11, 15, 13, 18, 23, 20, 26, 22, 13, 22, 22, 16, 11, 15, 13, 19, 16, 16, 26, 23, 13, 19, 21, 21, 11, 15, 13, 19, 19, 24, 26, 24, 13, 16, 17, 23, 11, 15, 13, 20, 15, 16, 26, 24, 13, 22, 18, 11, 15, 13, 20, 22, 26, 16, 15, 13, 17, 19, 18, 11, 15, 13, 21, 21, 26, 16, 15, 13, 21, 18, 19, 11, 15, 13, 22, 22, 20, 26, 16, 15, 13, 23, 22, 17, 11, 15, 13, 24, 16, 22, 26, 16, 15, 13, 24, 18, 11, 16, 13, 15, 24, 16, 26, 16, 15, 13, 22, 23, 22, 11, 16, 13, 17, 24, 21, 26, 16, 15, 13, 19, 18, 19, 11, 16, 13, 20, 17, 24, 26, 24, 13, 23, 22, 23, 11, 16, 13, 22, 22, 24, 26, 24, 13, 16, 19, 24, 11, 17, 13, 15, 17, 24, 26, 23, 13, 17, 24, 20, 11, 17, 13, 17, 20, 24, 26, 22, 13, 18, 23, 19, 11, 17, 13, 19, 19, 20, 26, 21, 13, 19, 23, 19, 11, 17, 13, 20, 21, 21, 26, 20, 13, 21, 20, 11, 17, 13, 21, 16, 20, 26, 19, 13, 24, 17, 24, 11, 17, 13, 20, 23, 23, 26, 19, 13, 18, 18, 21, 11, 17, 13, 19, 24, 22, 26, 18, 13, 23, 22, 16, 11, 17, 13, 18, 21, 16, 26, 18, 13, 20, 17, 17, 11, 17, 13, 16, 24, 20, 26, 18, 13, 17, 22, 19, 11, 17, 13, 15, 16, 22, 26, 18, 13, 16, 16, 16, 11, 16, 13, 23, 18, 23, 26, 18, 13, 15, 17, 16, 11, 16, 13, 21, 21, 21, 26, 17, 13, 24, 24, 17, 11, 16, 13, 20, 15, 21, 26, 18, 13, 15, 16, 23, 11, 16, 13, 18, 21, 17, 26, 18, 13, 15, 24, 18, 11, 16, 13, 17, 18, 19, 26, 18, 13, 17, 16, 18, 11, 16, 13, 16, 17, 18, 26, 18, 13, 18, 22, 21, 11, 16, 13, 15, 17, 22, 26, 18, 13, 20, 22, 24, 11, 15, 13, 24, 19, 22, 26, 18, 13, 23, 17, 17, 11, 15, 13, 23, 23, 17, 26, 19, 13, 16, 15, 16, 11, 15, 13, 23, 17, 24, 26, 19, 13, 19, 16, 20, 11, 15, 13, 22, 23, 24, 26, 19, 13, 22, 21, 16, 11, 15, 13, 22, 21, 26, 20, 13, 16, 18, 19, 11, 15, 13, 22, 19, 17, 26, 20, 13, 20, 17, 23, 11, 15, 13, 22, 18, 21, 26, 20, 13, 24, 18, 20, 11, 15, 13, 22, 19, 26, 21, 13, 18, 19, 21, 11, 15, 13, 22, 20, 20, 26, 21, 13, 22, 20, 11, 15, 13, 22, 23, 17, 26, 22, 13, 16, 18, 19, 11, 15, 13, 23, 17, 16, 26, 22, 13, 19, 23, 17, 11, 15, 13, 23, 22, 18, 26, 22, 13, 22, 23, 17, 11, 15, 13, 24, 18, 23, 26, 23, 13, 15, 16, 21, 11, 16, 13, 15, 16, 21, 26, 23, 13, 16, 22, 18, 11, 16, 13, 16, 15, 23, 26, 23, 13, 17, 19, 16, 11, 16, 13, 17, 16, 18, 26, 23, 13, 17, 16, 17, 11, 16, 13, 18, 17, 24, 26, 23, 13, 15, 23, 21, 11, 16, 13, 19, 20, 17, 26, 22, 13, 23, 21, 20, 11, 16, 13, 20, 22, 23, 26, 22, 13, 20, 21, 16, 11, 16, 13, 22, 26, 22, 13, 16, 24, 11, 16, 13, 23, 16, 17, 26, 21, 13, 22, 22, 20, 11, 16, 13, 24, 15, 20, 26, 21, 13, 18, 18, 23, 11, 16, 13, 24, 22, 19, 26, 20, 13, 24, 15, 19, 11, 17, 13, 15, 16, 19, 26, 20, 13, 19, 24, 18, 11, 17, 13, 15, 17, 18, 26, 20, 13, 16, 17, 16, 11, 17, 13, 15, 15, 19, 26, 19, 13, 23, 15, 16, 11, 16, 13, 24, 20, 23, 26, 19, 13, 20, 18, 20, 11, 16, 13, 23, 24, 17, 26, 19, 13, 18, 17, 21, 11, 16, 13, 23, 16, 16, 26, 19, 13, 16, 22, 16, 11, 16, 13, 22, 17, 16, 26, 19, 13, 15, 21, 23, 11, 16, 13, 21, 17, 23, 26, 19, 13, 15, 16, 17, 11, 16, 13, 20, 18, 20, 26, 18, 13, 24, 24, 24, 11, 16, 13, 19, 19, 20, 26, 19, 13, 15, 17, 21, 11, 16, 13, 18, 21, 17, 26, 19, 13, 15, 24, 11, 16, 13, 17, 23, 20, 26, 19, 13, 16, 23, 22, 11, 16, 13, 17, 16, 22, 26, 19, 13, 18, 16, 21, 11, 16, 13, 16, 20, 23, 26, 19, 13, 19, 22, 18, 11, 16, 13, 16, 15, 22, 26, 19, 13, 21, 20, 19, 11, 16, 13, 15, 21, 21, 26, 19, 13, 23, 20, 23, 11, 16, 13, 15, 18, 19, 26, 20, 13, 15, 22, 24, 11, 16, 13, 15, 16, 16, 26, 20, 13, 18, 16, 19, 11, 15, 13, 24, 24, 22, 26, 20, 13, 20, 20, 21, 11, 15, 13, 24, 24, 17, 26, 20, 13, 23, 11, 15, 13, 24, 24, 21, 26, 21, 13, 15, 19, 11, 16, 13, 15, 15, 23, 26, 21, 13, 17, 21, 23, 11, 16, 13, 15, 18, 26, 21, 13, 19, 22, 22, 11, 16, 13, 15, 21, 26, 21, 13, 21, 20, 24, 11, 16, 13, 15, 24, 23]\n",
      "Length of the above token: 1182\n"
     ]
    }
   ],
   "source": [
    "# Print tokenised output for the first system\n",
    "print('Two examples of tokens from tokenised_data:')\n",
    "print('')\n",
    "print('Preprocessed data:')\n",
    "print(traject_scaled_string[3])\n",
    "print('')\n",
    "print('After tokenisation:')\n",
    "print(tokenised_data.iloc[3][\"input_ids\"].squeeze().tolist())  # Tokenised tensor\n",
    "print('Length of the above token:',len(tokenised_data.iloc[3][\"input_ids\"].squeeze().tolist()))  # Tokenised tensor\n",
    "print('')\n",
    "print('')\n",
    "print('Preprocessed data:')\n",
    "print(traject_scaled_string[990])\n",
    "print('')\n",
    "print('After tokenisation:')\n",
    "print(tokenised_data.iloc[990][\"input_ids\"].squeeze().tolist())\n",
    "print('Length of the above token:',len(tokenised_data.iloc[990][\"input_ids\"].squeeze().tolist()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary of <b>Part 2 (a)</b>:\n",
    "\n",
    "- Load the predator-prey dataset from the `lotka_volterra_data.h5` file.\n",
    "- Applied the LLMTIME preprocessing scheme, this includes:\n",
    "    - Scale the numaric values using the fucntion ` scaling_operator` (in file `preprocessor.py`).\n",
    "    - Round the values to a fixed number of decimal places (we have used 3 d.p.).\n",
    "    - Converted the sequences into formatted strings using:\n",
    "        - Commas to separate variables at each timestep.\n",
    "        - Semicolons to separate different timesteps.\n",
    "        - Collection of every system.\n",
    "- Tokenised the processed sequences using the Qwen2.5 tokeniser.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 (b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section aims to evaluate the performance of the untrained Qwen2.5-Instruct model's forecasting ability on the tokenised dataset, i.e. `tokenised_data` from Part 2(a) (previus section).\n",
    "\n",
    "Due to lack of computational power and limited number of FLOPS available for this project, we are not going to invest much time to this part. To save time and FLOPS we are only going to consider a subset of the dataset, $10\\%$ of our original dataset, i.e. 100 systems out of the available 1000.\n",
    "\n",
    "Approach to the problem:\n",
    "\n",
    "1. Select the first 100 system from our dataset.\n",
    "2. Select the first 50 data points from each of the 100 selected systems\n",
    "3. Tokenise the selected subset.\n",
    "4. Use `model.generate` to predict and decode the remaining 50 pairs of datapoints.\n",
    "5. Convert back to tameseries data.\n",
    "6. Visualise: select one of the predictions and plot them against actual data.\n",
    "7. Measure error predicted vs real data, for each of the 100 selected systems.\n",
    "\n",
    "After completing all the above steps, we can analyse and comment on the results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. <b>Select the first 100 system from our dataset.</b>\n",
    "2. <b>Select the first 50 data points from each of the 100 selected systems</b>\n",
    "\n",
    "The easiest approach is to work with `df_traj_scaled` (`pandas.DataFrame` format). Easy to select 100 system and cut off the last half of the data-points (prey, predator)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>system_id</th>\n",
       "      <th>time_step</th>\n",
       "      <th>prey</th>\n",
       "      <th>predator</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.757</td>\n",
       "      <td>4.116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.929</td>\n",
       "      <td>3.083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.698</td>\n",
       "      <td>2.232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.835</td>\n",
       "      <td>1.612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.261</td>\n",
       "      <td>1.188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>999</td>\n",
       "      <td>200.0</td>\n",
       "      <td>3.566</td>\n",
       "      <td>2.292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>999</td>\n",
       "      <td>200.0</td>\n",
       "      <td>3.787</td>\n",
       "      <td>2.132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>999</td>\n",
       "      <td>200.0</td>\n",
       "      <td>4.099</td>\n",
       "      <td>2.039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>999</td>\n",
       "      <td>200.0</td>\n",
       "      <td>4.466</td>\n",
       "      <td>2.020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>999</td>\n",
       "      <td>200.0</td>\n",
       "      <td>4.840</td>\n",
       "      <td>2.076</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       system_id  time_step   prey  predator\n",
       "0              0        0.0  3.757     4.116\n",
       "1              0        0.0  2.929     3.083\n",
       "2              0        0.0  2.698     2.232\n",
       "3              0        0.0  2.835     1.612\n",
       "4              0        0.0  3.261     1.188\n",
       "...          ...        ...    ...       ...\n",
       "99995        999      200.0  3.566     2.292\n",
       "99996        999      200.0  3.787     2.132\n",
       "99997        999      200.0  4.099     2.039\n",
       "99998        999      200.0  4.466     2.020\n",
       "99999        999      200.0  4.840     2.076\n",
       "\n",
       "[100000 rows x 4 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_traj_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1018/2856005867.py:5: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_sub_traj_scaled = df_filtered.groupby('system_id').apply(lambda x: x.iloc[:-50]).reset_index(drop=True)\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Keep only the first 100 system_ids\n",
    "df_filtered = df_traj_scaled[df_traj_scaled['system_id'] < 100]  \n",
    "\n",
    "# Step 2: Remove last 50 rows for each system_id\n",
    "df_sub_traj_scaled = df_filtered.groupby('system_id').apply(lambda x: x.iloc[:-50]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>system_id</th>\n",
       "      <th>time_step</th>\n",
       "      <th>prey</th>\n",
       "      <th>predator</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.757</td>\n",
       "      <td>4.116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.929</td>\n",
       "      <td>3.083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.698</td>\n",
       "      <td>2.232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.835</td>\n",
       "      <td>1.612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.261</td>\n",
       "      <td>1.188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>99</td>\n",
       "      <td>18.181818</td>\n",
       "      <td>10.700</td>\n",
       "      <td>0.208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>99</td>\n",
       "      <td>18.181818</td>\n",
       "      <td>11.849</td>\n",
       "      <td>0.223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>99</td>\n",
       "      <td>18.181818</td>\n",
       "      <td>12.991</td>\n",
       "      <td>0.253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>99</td>\n",
       "      <td>18.181818</td>\n",
       "      <td>14.071</td>\n",
       "      <td>0.302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>99</td>\n",
       "      <td>18.181818</td>\n",
       "      <td>15.018</td>\n",
       "      <td>0.375</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      system_id  time_step    prey  predator\n",
       "0             0   0.000000   3.757     4.116\n",
       "1             0   0.000000   2.929     3.083\n",
       "2             0   0.000000   2.698     2.232\n",
       "3             0   0.000000   2.835     1.612\n",
       "4             0   0.000000   3.261     1.188\n",
       "...         ...        ...     ...       ...\n",
       "4995         99  18.181818  10.700     0.208\n",
       "4996         99  18.181818  11.849     0.223\n",
       "4997         99  18.181818  12.991     0.253\n",
       "4998         99  18.181818  14.071     0.302\n",
       "4999         99  18.181818  15.018     0.375\n",
       "\n",
       "[5000 rows x 4 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sub_traj_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. <b>Tokenise the selected subset.</b>\n",
    "\n",
    "Recall Part 2(a): before tokenisation we had to convert data into string, we used the function `array_to_string` from `preprocessor.py`. Ultimately we tokenised the converted dataset with the function `tokenize_time_series` from `qwen.py`. Here we are going to repeat the same process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system_id\n",
      "0     3.757,4.116;2.929,3.083;2.698,2.232;2.835,1.61...\n",
      "1     3.842,3.977;4.266,3.25;4.987,2.715;6.019,2.346...\n",
      "2     4.245,4.401;3.378,3.56;3.038,2.801;3.04,2.187;...\n",
      "3     4.115,4.567;2.628,4.406;1.746,3.903;1.283,3.28...\n",
      "4     3.276,3.167;3.533,2.247;4.089,1.611;4.943,1.17...\n",
      "                            ...                        \n",
      "95    4.234,3.744;2.521,3.788;1.537,3.401;1.033,2.84...\n",
      "96    3.475,3.839;3.859,2.855;4.615,2.16;5.79,1.687;...\n",
      "97    4.49,3.363;2.452,3.76;1.269,3.499;0.715,2.929;...\n",
      "98    4.154,3.333;3.803,3.131;3.619,2.883;3.602,2.63...\n",
      "99    3.52,4.021;2.318,2.788;1.856,1.846;1.713,1.202...\n",
      "Length: 100, dtype: object\n"
     ]
    }
   ],
   "source": [
    "from src.preprocessor import Preprocessor\n",
    "\n",
    "# Converting subset of Timeseries data into str  \n",
    "array_to_string = Preprocessor.array_to_string\n",
    "\n",
    "str_sub_traj_scaled = array_to_string(df_sub_traj_scaled)\n",
    "\n",
    "print(str_sub_traj_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system_id\n",
      "0     [input_ids, attention_mask]\n",
      "1     [input_ids, attention_mask]\n",
      "2     [input_ids, attention_mask]\n",
      "3     [input_ids, attention_mask]\n",
      "4     [input_ids, attention_mask]\n",
      "                 ...             \n",
      "95    [input_ids, attention_mask]\n",
      "96    [input_ids, attention_mask]\n",
      "97    [input_ids, attention_mask]\n",
      "98    [input_ids, attention_mask]\n",
      "99    [input_ids, attention_mask]\n",
      "Length: 100, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Tokenisation of str_sub_traj_scaled\n",
    "\n",
    "# Import required funstion\n",
    "from src.qwen import tokenize_time_series\n",
    "\n",
    "# Tokenising sub-dataset\n",
    "tok_sub_traj_scaled = tokenize_time_series(str_sub_traj_scaled)\n",
    "\n",
    "# Checking result\n",
    "print(tok_sub_traj_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below a basic function to determone the max lenght of a tokenised sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_sequence_length(tokenized_data):\n",
    "    max_length = max(entry[\"input_ids\"].shape[1] for entry in tokenized_data)\n",
    "\n",
    "    return f'Max Sequence Length: {max_length}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing full dataset vs sub-dataset max sequence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full dataset:\n",
      "Max Sequence Length: 1281\n",
      "\n",
      "Sub-dataset:\n",
      "Max Sequence Length: 633\n"
     ]
    }
   ],
   "source": [
    "print('Full dataset:')\n",
    "print(max_sequence_length(tokenised_data))\n",
    "print('')\n",
    "print('Sub-dataset:')\n",
    "print(max_sequence_length(tok_sub_traj_scaled))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, from the above output the largest sequence of each tokenised dataset has dropped by ~ $\\frac{1}{2}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. <b>Use `model.generate` to predict and decode the remaining 50 pairs of datapoints.</b>\n",
    "\n",
    "We need to know the maximum lenght for each sequence in the tokenised sub-datased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of sequence lengths:\n",
      "[609, 613, 608, 590, 631, 586, 613, 596, 583, 586, 592, 607, 588, 585, 595, 589, 596, 604, 594, 605, 585, 588, 589, 594, 605, 588, 588, 589, 595, 600, 596, 632, 591, 592, 605, 600, 598, 604, 633, 609, 593, 591, 592, 603, 605, 599, 606, 582, 613, 590, 593, 583, 624, 589, 614, 591, 590, 611, 589, 590, 598, 592, 620, 623, 600, 614, 606, 589, 583, 601, 587, 604, 594, 588, 590, 604, 597, 596, 610, 605, 588, 583, 617, 587, 601, 597, 630, 588, 589, 595, 593, 604, 601, 586, 616, 588, 625, 587, 588, 609]\n"
     ]
    }
   ],
   "source": [
    "max_lengths = []  \n",
    "range_model = 100\n",
    "\n",
    "for i in range(range_model):\n",
    "    input_ids = tok_sub_traj_scaled[i][\"input_ids\"]  # Extract the input IDs\n",
    "    max_length = input_ids.shape[1]  # Get the length of the sequence\n",
    "    max_lengths.append(max_length)  # Append only the integer value\n",
    "print('List of sequence lengths:')\n",
    "print(max_lengths)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collecting `input_ids` and `attention_mask` into `input_ids_list` and `attention_mask_list` respectivelly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids_list = [tok_sub_traj_scaled[i][\"input_ids\"] for i in range(range_model)]\n",
    "attention_masks_list = [tok_sub_traj_scaled[i][\"attention_mask\"] for i in range(range_model)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we are using `model.generate` to predict the next 50 pairs (prey, predator), using the intrained Qwen2.5-Instruct model's forecasting ability on our tokenised data subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_list = []\n",
    "\n",
    "for i in range(range_model):\n",
    "    output = model.generate(input_ids_list[i], # Sequence, numerical tokens\n",
    "                            attention_mask = attention_masks_list[i], # \n",
    "                            max_length = 2*max_lengths[i], # Maximum length of generated output\n",
    "                            num_return_sequences=1, # Number of prediction to return per input\n",
    "                            do_sample=True, # Enables sampling (i.e. introduces randomness)\n",
    "                            temperature=0.7, # Controls randomness of predictions\n",
    "                            top_p=0.8, # Nucleus sampling (filters low-probability tokens)\n",
    "                            top_k=20 # Limits the number of token options at each step\n",
    "                            )\n",
    "    output_list.append(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[18, 13, 22,  ..., 22, 16, 18]]),\n",
       " tensor([[18, 13, 23,  ..., 23, 13, 22]]),\n",
       " tensor([[19, 13, 17,  ..., 26, 22, 13]]),\n",
       " tensor([[19, 13, 16,  ..., 22, 22, 16]]),\n",
       " tensor([[18, 13, 17,  ..., 19, 19, 26]]),\n",
       " tensor([[18, 13, 19,  ..., 22, 20, 23]]),\n",
       " tensor([[18, 13, 17,  ..., 22, 13, 16]]),\n",
       " tensor([[18, 13, 17,  ..., 16, 11, 19]]),\n",
       " tensor([[19, 13, 18,  ..., 26, 18, 13]]),\n",
       " tensor([[19, 13, 20,  ..., 18, 26, 15]]),\n",
       " tensor([[    18,     13,     23,     17,     18,     11,     19,     13,     18,\n",
       "              18,     26,     17,     13,     24,     24,     23,     11,     18,\n",
       "              13,     22,     19,     24,     26,     17,     13,     21,     17,\n",
       "              21,     11,     18,     13,     16,     17,     18,     26,     17,\n",
       "              13,     20,     21,     21,     11,     17,     13,     20,     21,\n",
       "              21,     26,     17,     13,     22,     20,     11,     17,     13,\n",
       "              16,     16,     23,     26,     18,     13,     16,     21,     17,\n",
       "              11,     16,     13,     22,     23,     18,     26,     18,     13,\n",
       "              23,     16,     24,     11,     16,     13,     20,     20,     18,\n",
       "              26,     19,     13,     22,     19,     11,     16,     13,     19,\n",
       "              17,     18,     26,     20,     13,     24,     17,     20,     11,\n",
       "              16,     13,     18,     24,     18,     26,     22,     13,     18,\n",
       "              16,     24,     11,     16,     13,     19,     22,     19,     26,\n",
       "              23,     13,     22,     18,     24,     11,     16,     13,     21,\n",
       "              24,     17,     26,     24,     13,     23,     20,     16,     11,\n",
       "              17,     13,     15,     23,     21,     26,     16,     15,     13,\n",
       "              17,     17,     19,     11,     17,     13,     21,     22,     22,\n",
       "              26,     24,     13,     20,     16,     19,     11,     18,     13,\n",
       "              19,     15,     21,     26,     22,     13,     23,     23,     16,\n",
       "              11,     19,     13,     15,     21,     17,     26,     20,     13,\n",
       "              24,     23,     19,     11,     19,     13,     18,     22,     22,\n",
       "              26,     19,     13,     19,     20,     17,     11,     19,     13,\n",
       "              17,     19,     24,     26,     18,     13,     19,     24,     11,\n",
       "              18,     13,     23,     16,     18,     26,     17,     13,     24,\n",
       "              24,     21,     11,     18,     13,     17,     21,     22,     26,\n",
       "              17,     13,     23,     19,     17,     11,     17,     13,     22,\n",
       "              19,     16,     26,     17,     13,     24,     19,     21,     11,\n",
       "              17,     13,     17,     24,     23,     26,     18,     13,     17,\n",
       "              22,     24,     11,     16,     13,     24,     20,     19,     26,\n",
       "              18,     13,     23,     19,     16,     11,     16,     13,     22,\n",
       "              16,     26,     19,     13,     21,     19,     17,     11,     16,\n",
       "              13,     20,     21,     19,     26,     20,     13,     21,     22,\n",
       "              21,     11,     16,     13,     20,     16,     20,     26,     21,\n",
       "              13,     23,     24,     19,     11,     16,     13,     20,     22,\n",
       "              16,     26,     23,     13,     16,     19,     19,     11,     16,\n",
       "              13,     22,     20,     17,     26,     24,     13,     16,     21,\n",
       "              19,     11,     17,     13,     15,     23,     19,     26,     24,\n",
       "              13,     21,     15,     22,     11,     17,     13,     20,     23,\n",
       "              16,     26,     24,     13,     16,     22,     24,     11,     18,\n",
       "              13,     16,     24,     24,     26,     22,     13,     24,     18,\n",
       "              19,     11,     18,     13,     22,     23,     19,     26,     21,\n",
       "              13,     18,     17,     17,     11,     19,     13,     16,     17,\n",
       "              20,     26,     19,     13,     23,     23,     18,     11,     19,\n",
       "              13,     16,     15,     17,     26,     18,     13,     23,     23,\n",
       "              23,     11,     18,     13,     22,     23,     16,     26,     18,\n",
       "              13,     18,     18,     16,     11,     18,     13,     18,     16,\n",
       "              23,     26,     18,     13,     16,     16,     18,     11,     17,\n",
       "              13,     23,     19,     26,     18,     13,     16,     21,     11,\n",
       "              17,     13,     19,     16,     24,     26,     18,     13,     19,\n",
       "              18,     20,     11,     17,     13,     15,     23,     17,     26,\n",
       "              18,     13,     24,     17,     24,     11,     16,     13,     23,\n",
       "              18,     22,     26,     19,     13,     21,     19,     19,     11,\n",
       "              16,     13,     21,     23,     20,     26,     20,     13,     20,\n",
       "              21,     20,     11,     16,     13,     21,     17,     21,     26,\n",
       "              21,     13,     21,     19,     17,     11,     16,     13,     21,\n",
       "              21,     23,     26,     22,     13,     22,     19,     21,     11,\n",
       "              16,     13,     23,     17,     21,     26,     23,     13,     21,\n",
       "              20,     20,     11,     17,     13,     16,     16,     23,     26,\n",
       "              24,     13,     15,     23,     24,     11,     17,     13,     20,\n",
       "              19,     24,     26,     23,     13,     23,     15,     17,     11,\n",
       "              18,     13,     15,     23,     17,     26,     22,     13,     23,\n",
       "              16,     17,     11,     18,     13,     20,     24,     20,     26,\n",
       "              21,     13,     19,     19,     21,     11,     18,     13,     24,\n",
       "              17,     26,     20,     13,     16,     19,     19,     11,     18,\n",
       "              13,     24,     19,     22,     26,     19,     13,     16,     23,\n",
       "              18,     11,     18,     13,     22,     15,     19,     26,     18,\n",
       "              13,     17,     22,     18,     11,     18,     13,     18,     24,\n",
       "              20,     26,     17,     13,     18,     24,     11,     17,     13,\n",
       "              23,     20,     19,     26,     16,     13,     18,     16,     11,\n",
       "              17,     13,     18,     16,     17,     26,     15,     13,     20,\n",
       "              21,     11,     16,     13,     23,     18,     22,     26,     15,\n",
       "              13,     17,     19,     11,     16,     13,     17,     24,     19,\n",
       "              26,     15,     13,     15,     21,     11,     16,     13,     16,\n",
       "              17,     18,     26,     15,     13,     15,     18,     11,     16,\n",
       "              13,     15,     19,     16,     26,     15,     13,     15,     16,\n",
       "              11,     16,     13,     15,     15,     20,     26,     15,     13,\n",
       "              15,     15,     11,     15,     13,     24,     24,     24,     26,\n",
       "              15,     13,     15,     15,     11,     15,     13,     24,     24,\n",
       "              23,     60,   3989,  73594,  12669,    198,    474,   8591,    438,\n",
       "            2595,    271,      2,  18614,    279,    729,    311,    387,  81078,\n",
       "             198,    750,    282,   2075,    982,    262,    470,    320,     87,\n",
       "              58,     15,  78341,     17,    488,    856,     58,     16,  78341,\n",
       "              17,  32295,     15,     13,     20,    271,      2,  18614,    279,\n",
       "           21568,    198,  48057,    284,  17826,     87,     58,     15,   1125,\n",
       "             220,     16,    701,    320,     87,     58,     16,   1125,    220,\n",
       "              16,  27771,      2,   5443,  28090,  74497,   4358,  11853,    311,\n",
       "            1477,    279,   8028,    897,    315,    279,    729,   3832,    311,\n",
       "             279,  16982,    198,   1382,    284,   1308,  11853,    955,     11,\n",
       "             508,     15,     11,    220,     15,   1125,  16982,     28,  48057,\n",
       "             692,      2,    576,   1102,    374,    304,    264,   1352,    429,\n",
       "             646,    387,  67583,    198,   1350,   4456,   1993,    340,  13874,\n",
       "           19324,   1986,   2038,  43065,  18653,    264,    729,   1565,     69,\n",
       "              63,    429,  10868,    279,   9334,   3704,    315,    279,   2629,\n",
       "             315,  31340,    315,   1378,   7332,    323,   1221,   5711,   1565,\n",
       "            2388,  22947,  74497,   4358,  11853,     63,    311,  29337,    419,\n",
       "             729,    448,   5091,    311,    279,   2661,  16982,     13,    576,\n",
       "            3059,    525,  16709,    700,     11,    892,   1265,   2968,    498,\n",
       "             279,   2750,    315,  17767,    856,     62,     16,   1124,      8,\n",
       "             323,  17767,    856,     62,     17,   1124,      8,    429,  29337,\n",
       "             279,   6010,    504,    279,   6238,   1212,    279,   5189,  16982,\n",
       "              13,   7036,    429,   2474,    582,   2299,  76291,    916,    678,\n",
       "            3204,   3501,   2878,    279,   4982,  12671,     11,    582,   1513,\n",
       "             944,    614,    458,  11464,  14806,    369,    279,   6291,    714,\n",
       "            4751,    458,  44868,   4226,   3118,    389,    279,  34776,  25262,\n",
       "            1882,    382,   2679,    498,   1184,    279,   4734,  42964,   9904,\n",
       "             476,   4623,   3565,    911,   1246,    311,  14198,   1493,   3059,\n",
       "              11,   4486,   1077,    752,   1414,      0,   6771,    594,  10354,\n",
       "             448,   9271,    279,   6291,    624,  73594, 151643]]),\n",
       " tensor([[19, 13, 21,  ..., 17, 11, 15]]),\n",
       " tensor([[19, 13, 20,  ..., 26, 17, 13]]),\n",
       " tensor([[19, 13, 20,  ..., 15, 16, 26]]),\n",
       " tensor([[18, 13, 22,  ..., 16, 24, 13]]),\n",
       " tensor([[19, 13, 21,  ..., 21, 13, 15]]),\n",
       " tensor([[19, 13, 18,  ..., 13, 22, 21]]),\n",
       " tensor([[18, 13, 22,  ..., 16, 26, 15]]),\n",
       " tensor([[19, 13, 20,  ..., 26, 17, 13]]),\n",
       " tensor([[18, 13, 16,  ..., 20, 26, 18]]),\n",
       " tensor([[19, 13, 15,  ..., 11, 17, 17]]),\n",
       " tensor([[18, 13, 24,  ..., 22, 26, 20]]),\n",
       " tensor([[18, 13, 17,  ..., 22, 11, 15]]),\n",
       " tensor([[19, 13, 20,  ..., 15, 15, 15]]),\n",
       " tensor([[18, 13, 21,  ..., 15, 13, 15]]),\n",
       " tensor([[18, 13, 22,  ..., 16, 11, 16]]),\n",
       " tensor([[18, 13, 24,  ..., 24, 13, 18]]),\n",
       " tensor([[19, 13, 19,  ..., 21, 21, 20]]),\n",
       " tensor([[19, 13, 16,  ..., 24, 11, 17]]),\n",
       " tensor([[19, 13, 15,  ..., 15, 11, 15]]),\n",
       " tensor([[18, 13, 22,  ..., 16, 13, 18]]),\n",
       " tensor([[18, 13, 21,  ..., 11, 15, 13]]),\n",
       " tensor([[18, 13, 24,  ..., 20, 23, 16]]),\n",
       " tensor([[18, 13, 20,  ..., 15, 13, 17]]),\n",
       " tensor([[18, 13, 22,  ..., 18, 23, 11]]),\n",
       " tensor([[18, 13, 23,  ..., 18, 11, 19]]),\n",
       " tensor([[19, 13, 19,  ..., 18, 17, 20]]),\n",
       " tensor([[19, 13, 21,  ..., 11, 20, 13]]),\n",
       " tensor([[19, 13, 19,  ..., 18, 22, 11]]),\n",
       " tensor([[18, 13, 24,  ..., 19, 18, 23]]),\n",
       " tensor([[19, 13, 17,  ..., 16, 20, 11]]),\n",
       " tensor([[18, 13, 21,  ..., 24, 19, 26]]),\n",
       " tensor([[18, 13, 23,  ..., 24, 24, 20]]),\n",
       " tensor([[19, 13, 21,  ..., 17, 17, 13]]),\n",
       " tensor([[18, 13, 22,  ..., 11, 17, 13]]),\n",
       " tensor([[18, 13, 19,  ..., 24, 13, 22]]),\n",
       " tensor([[18, 13, 24,  ..., 15, 17, 26]]),\n",
       " tensor([[19, 13, 16,  ..., 21, 26, 15]]),\n",
       " tensor([[18, 13, 18,  ..., 15, 16, 11]]),\n",
       " tensor([[18, 13, 18,  ..., 16, 24, 13]]),\n",
       " tensor([[18, 13, 21,  ..., 21, 13, 23]]),\n",
       " tensor([[19, 13, 21,  ..., 26, 22, 13]]),\n",
       " tensor([[19, 13, 17,  ..., 16, 11, 15]]),\n",
       " tensor([[18, 13, 20,  ..., 22, 20, 26]]),\n",
       " tensor([[19, 13, 16,  ..., 20, 11, 15]]),\n",
       " tensor([[19, 13, 17,  ..., 24, 15, 20]]),\n",
       " tensor([[18, 13, 24,  ..., 26, 20, 13]]),\n",
       " tensor([[19, 13, 18,  ..., 16, 11, 15]]),\n",
       " tensor([[   18,    13,    17,  ..., 24968,    20,    22]]),\n",
       " tensor([[18, 13, 19,  ..., 17, 15, 13]]),\n",
       " tensor([[19, 13, 21,  ..., 13, 19, 15]]),\n",
       " tensor([[18, 13, 24,  ..., 16, 24, 13]]),\n",
       " tensor([[18, 13, 17,  ..., 11, 15, 13]]),\n",
       " tensor([[18, 13, 21,  ..., 13, 16, 17]]),\n",
       " tensor([[18, 13, 16,  ..., 15, 24, 21]]),\n",
       " tensor([[19, 13, 15,  ..., 15, 26, 20]]),\n",
       " tensor([[19, 13, 22,  ..., 13, 15, 15]]),\n",
       " tensor([[19, 13, 15,  ..., 15, 13, 15]]),\n",
       " tensor([[19, 13, 16,  ..., 11, 16, 13]]),\n",
       " tensor([[18, 13, 21,  ..., 26, 15, 13]]),\n",
       " tensor([[18, 13, 20,  ..., 22, 23, 16]]),\n",
       " tensor([[18, 13, 24,  ..., 16, 23, 13]]),\n",
       " tensor([[19, 13, 20,  ..., 16, 26, 21]]),\n",
       " tensor([[18, 13, 16,  ..., 18, 13, 21]]),\n",
       " tensor([[19, 13, 15,  ..., 20, 26, 15]]),\n",
       " tensor([[18, 13, 23,  ..., 21, 24, 19]]),\n",
       " tensor([[19, 13, 16,  ..., 13, 19, 26]]),\n",
       " tensor([[19, 13, 19,  ..., 13, 17, 18]]),\n",
       " tensor([[18, 13, 23,  ..., 15, 13, 17]]),\n",
       " tensor([[18, 13, 20,  ..., 21, 20, 26]]),\n",
       " tensor([[18, 13, 22,  ..., 16, 13, 23]]),\n",
       " tensor([[18, 13, 19,  ..., 16, 13, 16]]),\n",
       " tensor([[18, 13, 17,  ..., 17, 20, 26]]),\n",
       " tensor([[18, 13, 22,  ..., 15, 15, 16]]),\n",
       " tensor([[19, 13, 16,  ..., 16, 18, 26]]),\n",
       " tensor([[18, 13, 20,  ..., 18, 17, 13]]),\n",
       " tensor([[19, 13, 21,  ..., 18, 13, 24]]),\n",
       " tensor([[19, 13, 17,  ..., 19, 11, 17]]),\n",
       " tensor([[19, 13, 17,  ..., 13, 22, 23]]),\n",
       " tensor([[19, 13, 17,  ..., 11, 19, 13]]),\n",
       " tensor([[18, 13, 16,  ..., 13, 15, 17]]),\n",
       " tensor([[19, 13, 19,  ..., 23, 13, 15]]),\n",
       " tensor([[19, 13, 21,  ..., 17, 13, 15]]),\n",
       " tensor([[18, 13, 16,  ..., 13, 20, 15]]),\n",
       " tensor([[19, 13, 16,  ..., 19, 11, 16]]),\n",
       " tensor([[19, 13, 17,  ..., 23, 26, 19]]),\n",
       " tensor([[18, 13, 19,  ..., 21, 11, 16]]),\n",
       " tensor([[19, 13, 19,  ..., 16, 26, 20]]),\n",
       " tensor([[19, 13, 16,  ..., 19, 13, 22]]),\n",
       " tensor([[18, 13, 20,  ..., 17, 20, 21]])]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we are decoding the predicted output and collecting it into `decoded_output_list`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_output_list = []\n",
    "\n",
    "for i in range(range_model):\n",
    "    decoded_output = tokenizer.decode(output_list[i][0], skip_special_tokens=True)\n",
    "    decoded_output_list.append(decoded_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Converting strings output in `decoded_output_list` back into timeseries data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_to_array_list = []\n",
    "\n",
    "for i in range(range_model):\n",
    "    decoded_to_number = string_to_array(decoded_output_list[i])\n",
    "    string_to_array_list.append(decoded_to_number)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving encoded prediction, as to predict the next ~50 pair points in each of the 100 selected systems, required a run time of 140 minutes (~2h and 20 minutes). The file is saved as `my_decoded_predictions.npz`. I used `numpy.savez`, as `string_to_array_list` is a list of arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_decoded_predictions = string_to_array_list\n",
    "type(my_decoded_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.savez(\"my_decoded_predictions.npz\", *my_decoded_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded = np.load(\"my_decoded_predictions.npz\")\n",
    "my_decoded_predictions = [loaded[key] for key in loaded]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Determining the mean square error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our input for `model.generate` where the first 50 pair point (prey, predator), when calculating the mean square error (MSE), we can remove the first 50 datapoints from our prediced output, as model provides input tokens $+$ predicted tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the first 50 input tokens and true values\n",
    "\n",
    "predicted_output = []\n",
    "true_values = []\n",
    "for i in range(len(string_to_array_list)):\n",
    "    sub_output = string_to_array_list[i]\n",
    "    sub_output_min_50 = sub_output[50:]\n",
    "    true_v = trajectories_scaled[i]\n",
    "    true_v_min_50 = true_v[50:]\n",
    "    predicted_output.append(sub_output_min_50)\n",
    "    true_values.append(true_v_min_50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now another issue occurs, some of the predictions have more or less than 50 pair points, henche when determing the MSE we need to be careful in terms of dimensions. Furthermore we are going to use `mean_quare_error` from `sklearn.matrices` to compute the MSE between predicted and true values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>system_id</th>\n",
       "      <th>MSE for prey</th>\n",
       "      <th>MSE for predator</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>5.629125</td>\n",
       "      <td>8.468667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>6.507396</td>\n",
       "      <td>2.038146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1.792820</td>\n",
       "      <td>0.181000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3.144939</td>\n",
       "      <td>1.884898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.017673</td>\n",
       "      <td>0.009245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>95</td>\n",
       "      <td>23.856767</td>\n",
       "      <td>12.892744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>96</td>\n",
       "      <td>7.360580</td>\n",
       "      <td>1.778820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>97</td>\n",
       "      <td>0.252265</td>\n",
       "      <td>0.661653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>98</td>\n",
       "      <td>0.145360</td>\n",
       "      <td>1.691620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>99</td>\n",
       "      <td>29.935542</td>\n",
       "      <td>8.961104</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    system_id  MSE for prey  MSE for predator\n",
       "0           0      5.629125          8.468667\n",
       "1           1      6.507396          2.038146\n",
       "2           2      1.792820          0.181000\n",
       "3           3      3.144939          1.884898\n",
       "4           4      0.017673          0.009245\n",
       "..        ...           ...               ...\n",
       "95         95     23.856767         12.892744\n",
       "96         96      7.360580          1.778820\n",
       "97         97      0.252265          0.661653\n",
       "98         98      0.145360          1.691620\n",
       "99         99     29.935542          8.961104\n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing MSE\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "MSE_values = []\n",
    "\n",
    "\n",
    "for i in range(len(true_values)):\n",
    "\n",
    "    # Truncate to match the shorter list\n",
    "    min_length = min(len(predicted_output[i]), len(true_values[i]))\n",
    "    pr_out = predicted_output[i][:min_length]\n",
    "    true_val = true_values[i][:min_length]\n",
    "\n",
    "    # Compute the MSE\n",
    "    mse_prey = mean_absolute_error(pr_out[:,0], true_val[:,0]) # Computing MSE for prey\n",
    "    mse_predator = mean_absolute_error(pr_out[:,1], true_val[:,1]) # Computing MSE for predator\n",
    "    MSE_values.append([mse_prey, mse_predator])\n",
    "\n",
    "\n",
    "# Collecting everything into a dataframe\n",
    "# Create a DataFrame\n",
    "df_MSE_values = pd.DataFrame({\n",
    "    \"system_id\": np.arange(len(MSE_values)),  \n",
    "    \"MSE for prey\": np.array(MSE_values)[:, 0],  # Flatten prey values\n",
    "    \"MSE for predator\": np.array(MSE_values)[:, 1]  # Flatten predator values\n",
    "})\n",
    "df_MSE_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the above table as a `csv` file using `pandas`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_MSE_values.to_csv('mse_true_predicted.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load `mse_true_predicted` back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>system_id</th>\n",
       "      <th>MSE for prey</th>\n",
       "      <th>MSE for predator</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>5.629125</td>\n",
       "      <td>8.468667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>6.507396</td>\n",
       "      <td>2.038146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1.792820</td>\n",
       "      <td>0.181000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3.144939</td>\n",
       "      <td>1.884898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.017673</td>\n",
       "      <td>0.009245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>95</td>\n",
       "      <td>23.856767</td>\n",
       "      <td>12.892744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>96</td>\n",
       "      <td>7.360580</td>\n",
       "      <td>1.778820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>97</td>\n",
       "      <td>0.252265</td>\n",
       "      <td>0.661653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>98</td>\n",
       "      <td>0.145360</td>\n",
       "      <td>1.691620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>99</td>\n",
       "      <td>29.935542</td>\n",
       "      <td>8.961104</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    system_id  MSE for prey  MSE for predator\n",
       "0           0      5.629125          8.468667\n",
       "1           1      6.507396          2.038146\n",
       "2           2      1.792820          0.181000\n",
       "3           3      3.144939          1.884898\n",
       "4           4      0.017673          0.009245\n",
       "..        ...           ...               ...\n",
       "95         95     23.856767         12.892744\n",
       "96         96      7.360580          1.778820\n",
       "97         97      0.252265          0.661653\n",
       "98         98      0.145360          1.691620\n",
       "99         99     29.935542          8.961104\n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse_true_predicted = pd.read_csv(\"mse_true_predicted.csv\")\n",
    "mse_true_predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Visualising one of the 100 predicted systems vs its true values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99, 2)\n",
      "(100, 2)\n"
     ]
    }
   ],
   "source": [
    "index = 4\n",
    "traj_00 = string_to_array(traject_scaled_string[index])\n",
    "dec_00 = string_to_array_list[index]\n",
    "print(dec_00.shape)\n",
    "print(traj_00.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Ensure `decoded_to_number` is trimmed to match `traj_0`\n",
    "traj_0 = traj_00[50:]\n",
    "dec_0 = dec_00[50:]\n",
    "min_length = min(len(traj_0), len(dec_0))\n",
    "\n",
    "# Trim both arrays to the same length\n",
    "time_step = np.linspace(100, 200, min_length)  # Adjust time range\n",
    "prey_values = traj_0[:min_length, 0]  # Trim original prey\n",
    "predator_values = traj_0[:min_length, 1]  # Trim original predator\n",
    "\n",
    "predict_prey = dec_0[:min_length, 0]  # Trim predicted prey\n",
    "predict_predator = dec_0[:min_length, 1]  # Trim predicted predator\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(time_step, prey_values, label=\"Original Prey\", color=\"blue\", linestyle=\"dashed\")\n",
    "plt.plot(time_step, predator_values, label=\"Original Predator\", color=\"red\", linestyle=\"dashed\")\n",
    "\n",
    "plt.plot(time_step, predict_prey, label=\"Predicted Prey\", color=\"blue\")\n",
    "plt.plot(time_step, predict_predator, label=\"Predicted Predator\", color=\"red\")\n",
    "\n",
    "# Labels and Title\n",
    "plt.xlabel(\"Time Steps\")\n",
    "plt.ylabel(\"Population\")\n",
    "plt.title(\"Prey-Predator Dynamics Over Time\")\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare input data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "m2_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
