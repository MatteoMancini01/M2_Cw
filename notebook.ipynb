{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required packages\n",
    "import torch\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open `lotka_volterra_data.h5` file on notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('lotka_volterra_data.h5', 'r') as f:\n",
    "    # Access the full dataset\n",
    "    trajectories = f['trajectories'][:]\n",
    "    time_points = f['time'][:]\n",
    "\n",
    "    # Access a single trajectory\n",
    "    system_id = 0 # First system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checikng shape of the dataset, we expect trajectories to be of size $(1000 \\times 100 \\times 2)$, and time_points of size $(100 \\times 1)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time points shape: (100,)\n",
      "\n",
      "Trajectory shape (pray/predator): (1000, 100, 2)\n"
     ]
    }
   ],
   "source": [
    "print('Time points shape:',time_points.shape)\n",
    "print('')\n",
    "print('Trajectory shape (pray/predator):',trajectories.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_systems, num_time_steps, num_variables = trajectories.shape\n",
    "# Create a DataFrame\n",
    "df_traj = pd.DataFrame({\n",
    "    \"system_id\": np.repeat(np.arange(num_systems), num_time_steps),  # Repeats 0-999, each 100 times\n",
    "    \"time_step\": np.repeat(time_points[np.arange(num_time_steps)], num_systems),    # Cycles 0-99 for each system\n",
    "    \"prey\": trajectories[:, :, 0].flatten(),  # Flatten prey values\n",
    "    \"predator\": trajectories[:, :, 1].flatten()  # Flatten predator values\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>system_id</th>\n",
       "      <th>time_step</th>\n",
       "      <th>prey</th>\n",
       "      <th>predator</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.949917</td>\n",
       "      <td>1.040624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.740551</td>\n",
       "      <td>0.779542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.682246</td>\n",
       "      <td>0.564390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.716674</td>\n",
       "      <td>0.407644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.824511</td>\n",
       "      <td>0.300283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>999</td>\n",
       "      <td>200.0</td>\n",
       "      <td>0.901549</td>\n",
       "      <td>0.579420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>999</td>\n",
       "      <td>200.0</td>\n",
       "      <td>0.957527</td>\n",
       "      <td>0.539055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>999</td>\n",
       "      <td>200.0</td>\n",
       "      <td>1.036460</td>\n",
       "      <td>0.515615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>999</td>\n",
       "      <td>200.0</td>\n",
       "      <td>1.129212</td>\n",
       "      <td>0.510619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>999</td>\n",
       "      <td>200.0</td>\n",
       "      <td>1.223701</td>\n",
       "      <td>0.524988</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       system_id  time_step      prey  predator\n",
       "0              0        0.0  0.949917  1.040624\n",
       "1              0        0.0  0.740551  0.779542\n",
       "2              0        0.0  0.682246  0.564390\n",
       "3              0        0.0  0.716674  0.407644\n",
       "4              0        0.0  0.824511  0.300283\n",
       "...          ...        ...       ...       ...\n",
       "99995        999      200.0  0.901549  0.579420\n",
       "99996        999      200.0  0.957527  0.539055\n",
       "99997        999      200.0  1.036460  0.515615\n",
       "99998        999      200.0  1.129212  0.510619\n",
       "99999        999      200.0  1.223701  0.524988\n",
       "\n",
       "[100000 rows x 4 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_traj # Visualising data in dataframe format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.080808080808081\n"
     ]
    }
   ],
   "source": [
    "time_step = df_traj['time_step'].to_numpy()\n",
    "\n",
    "print(time_step[4925])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 (a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grouping prey and predator into arrays to determine the maximum value for scaling procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "prey_array = df_traj['prey'].to_numpy() # Converting to numpy array\n",
    "predator_array = df_traj['predator'].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.10 Scaling Dataset `lotka_volterra_data.h5`\n",
    "\n",
    "As we will see in the `Table` presented below, in the original dataset we have laues that vary significantly. To standardize the numeric range, we are going to use [quantiles]( https://en.wikipedia.org/wiki/Quantile). A quantile is a value that divides a dataset into equal-sized intervals, indicating the data points below which a given percentage if observations fall. From the project instructions it is adviced to apply a simple scaling:\n",
    "$$\n",
    "x_t' = \\frac{x_t}{\\alpha}\n",
    "$$\n",
    "where $\\alpha$ should be chosen based on the distribution of the dataset `lotka_volterra_data.h5`.\n",
    "\n",
    "In our particular case we want most of our dataset to be in range $[0,10]$. This is coded in the [`preprocessor.py`](https://github.com/MatteoMancini01/M2_Cw/blob/main/src/preprocessor.py) file, which appropriate docstrings.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `numpy.quantile()`\n",
    "\n",
    "For scaling our dataset we want to use [`numpy.quantile()`](https://numpy.org/doc/2.1/reference/generated/numpy.quantile.html). The `numpy.quantile()` function calculates the quantiles of a given NumPy array. Quantiles are cut points that devide the data into intercals with equal probability. Thus `numpy.quantile()`can be used to scale our dataset dynamically, without having to worry about choosing the appropriate value for $\\alpha$.\n",
    "\n",
    "All of this is implemented in the function `scaling_operator`, which also <b>rounds</b> each datapoint using `numpy.round()`, this is set to 3 decimal places."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import class Preprocessor from src/preprocessor.py\n",
    "from src.preprocessor import Preprocessor\n",
    "\n",
    "# Set scaling_operator to function \n",
    "scaling_operator = Preprocessor.scaling_operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaling factor: 0.25283724\n"
     ]
    }
   ],
   "source": [
    "trajectories_scaled, scaling_factor = scaling_operator(trajectories, 0.9, 10)\n",
    "print('Scaling factor:', scaling_factor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collecting scaled data into `pandas.DataFrame` format, in particular, we want to construct a $100000\\times 4$ table, (number of rows $= 1000 \\times 100$). With four colums, of which three are `time_step`, `prey` and `predator`, but with an additional one `system_id` (this separates the $1000$ different systems), which will be later used to convert our timeseries data into string format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_systems_scaled, num_time_steps_scaled, num_variables_scaled = trajectories_scaled.shape\n",
    "\n",
    "# Create a DataFrame\n",
    "df_traj_scaled = pd.DataFrame({\n",
    "    \"system_id\": np.repeat(np.arange(num_systems_scaled), num_time_steps_scaled),  # Repeats 0-999, each 100 times\n",
    "    \"time_step\": np.repeat(time_points[np.arange(num_time_steps_scaled)], num_systems_scaled),  # Cycles 0-200 (array.shape = (100,)) for each system\n",
    "    \"prey\": trajectories_scaled[:, :, 0].flatten(),  # Flatten prey values\n",
    "    \"predator\": trajectories_scaled[:, :, 1].flatten()  # Flatten predator values\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualising `df_traj_scaled`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>system_id</th>\n",
       "      <th>time_step</th>\n",
       "      <th>prey</th>\n",
       "      <th>predator</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.757</td>\n",
       "      <td>4.116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.929</td>\n",
       "      <td>3.083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.698</td>\n",
       "      <td>2.232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.835</td>\n",
       "      <td>1.612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.261</td>\n",
       "      <td>1.188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>999</td>\n",
       "      <td>200.0</td>\n",
       "      <td>3.566</td>\n",
       "      <td>2.292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>999</td>\n",
       "      <td>200.0</td>\n",
       "      <td>3.787</td>\n",
       "      <td>2.132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>999</td>\n",
       "      <td>200.0</td>\n",
       "      <td>4.099</td>\n",
       "      <td>2.039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>999</td>\n",
       "      <td>200.0</td>\n",
       "      <td>4.466</td>\n",
       "      <td>2.020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>999</td>\n",
       "      <td>200.0</td>\n",
       "      <td>4.840</td>\n",
       "      <td>2.076</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       system_id  time_step   prey  predator\n",
       "0              0        0.0  3.757     4.116\n",
       "1              0        0.0  2.929     3.083\n",
       "2              0        0.0  2.698     2.232\n",
       "3              0        0.0  2.835     1.612\n",
       "4              0        0.0  3.261     1.188\n",
       "...          ...        ...    ...       ...\n",
       "99995        999      200.0  3.566     2.292\n",
       "99996        999      200.0  3.787     2.132\n",
       "99997        999      200.0  4.099     2.039\n",
       "99998        999      200.0  4.466     2.020\n",
       "99999        999      200.0  4.840     2.076\n",
       "\n",
       "[100000 rows x 4 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_traj_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting `prey` and `predator` columns into array using [`pandas.DataFrame.to_numpy`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_numpy.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "prey_array_scaled = df_traj_scaled['prey'].to_numpy() # Converting to numpy array\n",
    "predator_array_scaled = df_traj_scaled['predator'].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining a function that calculates the percentage of values in an array that fall outside a given range. (This seem tedious, as we set a value for quantile in the function `scaling_operator`, e.g. $q = 0.9$, means that only $10%$ of the values will be out of our custom range. But this will be used to measure what percentage of datapoints in the original dataset is outside a specific range.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaling_measure(arr, min_val, max_val):\n",
    "    \n",
    "    \"\"\"\n",
    "    Calculates the percentage of values in an array that fall outside a given range.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    arr : array-like\n",
    "        The input numerical data.\n",
    "    min_val : float\n",
    "        The minimum acceptable value.\n",
    "    max_val : float\n",
    "        The maximum acceptable value.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    str\n",
    "        The percentage of values outside the range, formatted as a string.\n",
    "    \"\"\"\n",
    "\n",
    "    # Count values about the max range\n",
    "    outside_count = np.sum((arr < min_val)|(arr > max_val))\n",
    "\n",
    "    # Calculating the pergentage of values outside max range\n",
    "    percentage_outside = (outside_count/arr.size)*100\n",
    "\n",
    "    return f'{percentage_outside:.2f}%'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collecting scaling information into a Table using `pandas.DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_val = 0\n",
    "max_val = 1\n",
    "Table_1 = pd.DataFrame({\n",
    "\n",
    "    'Pray': [max(prey_array), np.mean(prey_array), min(prey_array), scaling_measure(prey_array, min_val, max_val)],\n",
    "    'Pray after scaling': [max(prey_array_scaled), np.mean(prey_array_scaled), min(prey_array_scaled), scaling_measure(prey_array_scaled, min_val, max_val)],\n",
    "    'Predator': [max(predator_array), np.mean(predator_array), min(predator_array), scaling_measure(predator_array, min_val, max_val)],\n",
    "    'Predator after scaling': [max(predator_array_scaled), np.mean(predator_array_scaled), min(predator_array_scaled), scaling_measure(predator_array_scaled, min_val, max_val)],\n",
    "    \n",
    "})\n",
    "Table_1.index = [\"Maximim Value\", \"Mean Value\", \"Minimum Value\", f\"Values outside the range {min_val}-{max_val}\"] # Adding index for each row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " From the table below, we can observe, scaling was successful. The reason why we want to test how many data points are outside the range $[0,1]$, is due to the fact that a lot of data points in the original dataset (pre-scaling) are very small, many of order $10^{-3}$ (and smaller order $10^{-4}$), which may affect the tokenisation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pray</th>\n",
       "      <th>Pray after scaling</th>\n",
       "      <th>Predator</th>\n",
       "      <th>Predator after scaling</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Maximim Value</th>\n",
       "      <td>13.740113</td>\n",
       "      <td>54.344002</td>\n",
       "      <td>4.76849</td>\n",
       "      <td>18.860001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mean Value</th>\n",
       "      <td>1.698114</td>\n",
       "      <td>6.71623</td>\n",
       "      <td>0.569606</td>\n",
       "      <td>2.252858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Minimum Value</th>\n",
       "      <td>0.002077</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Values outside the range 0-1</th>\n",
       "      <td>63.11%</td>\n",
       "      <td>93.82%</td>\n",
       "      <td>12.21%</td>\n",
       "      <td>77.31%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   Pray Pray after scaling  Predator  \\\n",
       "Maximim Value                 13.740113          54.344002   4.76849   \n",
       "Mean Value                     1.698114            6.71623  0.569606   \n",
       "Minimum Value                  0.002077              0.008  0.000037   \n",
       "Values outside the range 0-1     63.11%             93.82%    12.21%   \n",
       "\n",
       "                             Predator after scaling  \n",
       "Maximim Value                             18.860001  \n",
       "Mean Value                                 2.252858  \n",
       "Minimum Value                                   0.0  \n",
       "Values outside the range 0-1                 77.31%  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Table_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the last row, we can see that we have a major improvement for both `prey` and `predator` categories, the percentage of values outside the range $[0,1]$ has increased in `prey` by ~ $30\\%$ and `predator` by ~ $65\\%$. Thus, scaling was successful. Now we can proceed with the next step, i.e. converting the scaled dataset to strings, for compatibility with [Qwen2.5]( https://github.com/QwenLM/Qwen2.5)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.11 Loading Qwen2.5\n",
    "\n",
    "Below a short demonstration on how to use `load_qwen()` from `src.qwen`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/Document/Term2/M2/M2_Cw/m2_venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-03-15 14:55:04.965622: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1742050505.131670    4443 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1742050505.177932    4443 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-15 14:55:05.602096: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    }
   ],
   "source": [
    "from src.qwen import load_qwen # Import load_qwen\n",
    "model, tokenizer = load_qwen() # set model = model and tokeinzer = tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying with examples provided at the end of project instructions, see [LLMTIME Preprocessing Scheme](https://github.com/MatteoMancini01/M2_Cw/blob/main/instructions/main.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16, 13, 17, 18]\n",
      "\n",
      "[16, 659, 220, 17, 220, 18]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer(\"1.23\", return_tensors=\"pt\")[\"input_ids\"].tolist()[0])\n",
    "print('')\n",
    "print(tokenizer(\"1 . 2 3\", return_tensors=\"pt\")[\"input_ids\"].tolist()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying to tokenise $[0.25,1.50;0.27,1.47;0.31,1.42]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15, 13, 17, 20, 11, 16, 13, 20, 15, 26, 15, 13, 17, 22, 11, 16, 13, 19, 22, 26, 15, 13, 18, 16, 11, 16, 13, 19, 17]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer(\"0.25,1.50;0.27,1.47;0.31,1.42\", return_tensors='pt')[\"input_ids\"].tolist()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of how `load_qwen()` works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    }
   ],
   "source": [
    "text = 'Hello, world' # Define input text\n",
    "input_ids = tokenizer(text, return_tensors='pt').input_ids # Tokenize text \n",
    "output = model.generate(input_ids, max_length = 50) # Generate output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above code line 3 `output`, the variable `max_length` determines how many more words will the model predict when inputing text, e.g. `text = Hello, world`, as we can see from the below output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 9707,    11,  1879,     0,  1096,   374,   264,  4285, 13027,  5316,\n",
       "           429, 15804,   279,  8794,   315,   264,  1034,   323, 23473,   700,\n",
       "          1817,  1555,   304,  9931,  1973,    13,  5692,   594,  1246,   498,\n",
       "           646,   653,   432,  1447, 73594, 12669,   198,     2,  5264,   279,\n",
       "          1034,   304,  1349,  3856,   198,  4197,  1787,   492,  8404,  3909]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above tensor we recognise that the first 3 tokens are related to our text, the rest of the tokens is predicted text determined from the model, as we will see below when decoding `output`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, world! This is a simple Python script that reads the contents of a file and prints out each line in reverse order. Here's how you can do it:\n",
      "\n",
      "```python\n",
      "# Open the file in read mode\n",
      "with open('filename.txt\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(output[0], skip_special_tokens=True)) # Decoding output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.12 Converting Scaled Dataset into Strings\n",
    "\n",
    "We now have seen how tokenisation for text works! There is a small issue, Qwen2.5 is only designed to convert text, i.e. strings in Python, to tokens, while our dataset is a timeseries composed of 2 variables prey and predator over a time series of size 100, this is repated for a 1000 samples. Thus, before we proceed with tokenisation, we require to convert the time series data into sets of strings. To do so we are going to define a fucntion `array_to_string(data)`, and a function to convert string to array `sring_to_array(formatted_string)` (both functions are in [preprocessor.py](https://github.com/MatteoMancini01/M2_Cw/blob/main/src/preprocessor.py))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "âš  Note: The function `array_to_string` is specifically designed for the dataset `lotka_volterra_data.h5`, in particular after converting `trajectories` into a `panda.DataFrame` format, with columns `system_id` (labeling each system from 0 to 999), columns `prey` and `predator`, each  displaying 100 data points for every `system_id`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.preprocessor import Preprocessor\n",
    "\n",
    "array_to_string = Preprocessor.array_to_string # Importing array_to_string(data) to convert timeseries to string\n",
    "string_to_array = Preprocessor.string_to_array # Importing string_to_array(formatted_string) to convert strings back to arrays\n",
    "\n",
    "traject_scaled_string = array_to_string(df_traj_scaled) # Converting df_traj_scaled into string format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking result post-conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system_id\n",
      "0      3.757,4.116;2.929,3.083;2.698,2.232;2.835,1.61...\n",
      "1      3.842,3.977;4.266,3.25;4.987,2.715;6.019,2.346...\n",
      "2      4.245,4.401;3.378,3.56;3.038,2.801;3.04,2.187;...\n",
      "3      4.115,4.567;2.628,4.406;1.746,3.903;1.283,3.28...\n",
      "4      3.276,3.167;3.533,2.247;4.089,1.611;4.943,1.17...\n",
      "                             ...                        \n",
      "995    3.918,4.625;2.139,3.254;1.521,2.104;1.327,1.32...\n",
      "996    3.595,4.653;2.249,3.797;1.681,2.896;1.484,2.15...\n",
      "997    4.465,4.433;4.055,4.03;3.9,3.615;3.981,3.237;4...\n",
      "998    4.476,4.017;3.167,3.167;2.564,2.341;2.362,1.68...\n",
      "999    4.035,4.489;3.028,4.1;2.481,3.453;2.277,2.794;...\n",
      "Length: 1000, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(traject_scaled_string) # Print output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also want to test the function `string_to_array`, this is done below for the first `system_id` string data format, i.e. `system_id` $ = 0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3.757  4.116]\n",
      " [ 2.929  3.083]\n",
      " [ 2.698  2.232]\n",
      " [ 2.835  1.612]\n",
      " [ 3.261  1.188]\n",
      " [ 3.973  0.909]\n",
      " [ 4.995  0.736]\n",
      " [ 6.35   0.642]\n",
      " [ 8.033  0.611]\n",
      " [ 9.978  0.64 ]\n",
      " [12.027  0.741]\n",
      " [13.915  0.938]\n",
      " [15.283  1.271]\n",
      " [15.732  1.785]\n",
      " [14.933  2.492]\n",
      " [12.876  3.282]\n",
      " [10.089  3.882]\n",
      " [ 7.451  4.015]\n",
      " [ 5.574  3.666]\n",
      " [ 4.52   3.066]\n",
      " [ 4.08   2.448]\n",
      " [ 4.068  1.929]\n",
      " [ 4.385  1.536]\n",
      " [ 4.991  1.259]\n",
      " [ 5.869  1.081]\n",
      " [ 7.002  0.986]\n",
      " [ 8.344  0.963]\n",
      " [ 9.801  1.013]\n",
      " [11.212  1.145]\n",
      " [12.356  1.373]\n",
      " [12.979  1.715]\n",
      " [12.864  2.165]\n",
      " [11.941  2.672]\n",
      " [10.379  3.118]\n",
      " [ 8.59   3.351]\n",
      " [ 7.014  3.296]\n",
      " [ 5.897  3.004]\n",
      " [ 5.265  2.6  ]\n",
      " [ 5.044  2.193]\n",
      " [ 5.154  1.845]\n",
      " [ 5.539  1.577]\n",
      " [ 6.165  1.389]\n",
      " [ 6.997  1.277]\n",
      " [ 7.985  1.235]\n",
      " [ 9.047  1.263]\n",
      " [10.065  1.363]\n",
      " [10.892  1.539]\n",
      " [11.366  1.792]\n",
      " [11.36   2.108]\n",
      " [10.829  2.451]\n",
      " [ 9.87   2.75 ]\n",
      " [ 8.698  2.927]\n",
      " [ 7.569  2.934]\n",
      " [ 6.673  2.781]\n",
      " [ 6.095  2.527]\n",
      " [ 5.834  2.238]\n",
      " [ 5.856  1.969]\n",
      " [ 6.119  1.748]\n",
      " [ 6.586  1.585]\n",
      " [ 7.217  1.485]\n",
      " [ 7.96   1.445]\n",
      " [ 8.747  1.465]\n",
      " [ 9.484  1.546]\n",
      " [10.065  1.686]\n",
      " [10.389  1.879]\n",
      " [10.378  2.11 ]\n",
      " [10.015  2.349]\n",
      " [ 9.366  2.551]\n",
      " [ 8.563  2.672]\n",
      " [ 7.76   2.684]\n",
      " [ 7.09   2.589]\n",
      " [ 6.629  2.419]\n",
      " [ 6.4    2.215]\n",
      " [ 6.395  2.013]\n",
      " [ 6.589  1.84 ]\n",
      " [ 6.95   1.709]\n",
      " [ 7.44   1.626]\n",
      " [ 8.01   1.593]\n",
      " [ 8.602  1.611]\n",
      " [ 9.141  1.678]\n",
      " [ 9.552  1.791]\n",
      " [ 9.765  1.942]\n",
      " [ 9.735  2.115]\n",
      " [ 9.462  2.286]\n",
      " [ 8.994  2.426]\n",
      " [ 8.418  2.507]\n",
      " [ 7.838  2.513]\n",
      " [ 7.343  2.447]\n",
      " [ 6.995  2.326]\n",
      " [ 6.817  2.178]\n",
      " [ 6.812  2.029]\n",
      " [ 6.963  1.897]\n",
      " [ 7.244  1.795]\n",
      " [ 7.624  1.731]\n",
      " [ 8.061  1.707]\n",
      " [ 8.504  1.724]\n",
      " [ 8.898  1.78 ]\n",
      " [ 9.187  1.871]\n",
      " [ 9.324  1.988]\n",
      " [ 9.283  2.117]]\n"
     ]
    }
   ],
   "source": [
    "print(string_to_array(traject_scaled_string[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can observe from the above output, we successfully converted string back to array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.13 Tokenisation \n",
    "\n",
    "We provided few basic examples on how to use `load_qwen()` in section 2.11, with some text and numbers (string form). We now want to proceed and tokenise our data, to achieve this, we designed a function for our particular needs that uses `model, tokenizer = load_qwen()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.qwen import tokenize_time_series\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenised_data = tokenize_time_series(traject_scaled_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualising data before tokenisation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "system_id\n",
       "0      3.757,4.116;2.929,3.083;2.698,2.232;2.835,1.61...\n",
       "1      3.842,3.977;4.266,3.25;4.987,2.715;6.019,2.346...\n",
       "2      4.245,4.401;3.378,3.56;3.038,2.801;3.04,2.187;...\n",
       "3      4.115,4.567;2.628,4.406;1.746,3.903;1.283,3.28...\n",
       "4      3.276,3.167;3.533,2.247;4.089,1.611;4.943,1.17...\n",
       "                             ...                        \n",
       "995    3.918,4.625;2.139,3.254;1.521,2.104;1.327,1.32...\n",
       "996    3.595,4.653;2.249,3.797;1.681,2.896;1.484,2.15...\n",
       "997    4.465,4.433;4.055,4.03;3.9,3.615;3.981,3.237;4...\n",
       "998    4.476,4.017;3.167,3.167;2.564,2.341;2.362,1.68...\n",
       "999    4.035,4.489;3.028,4.1;2.481,3.453;2.277,2.794;...\n",
       "Length: 1000, dtype: object"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traject_scaled_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After tokenisation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "system_id\n",
       "0      [input_ids, attention_mask]\n",
       "1      [input_ids, attention_mask]\n",
       "2      [input_ids, attention_mask]\n",
       "3      [input_ids, attention_mask]\n",
       "4      [input_ids, attention_mask]\n",
       "                  ...             \n",
       "995    [input_ids, attention_mask]\n",
       "996    [input_ids, attention_mask]\n",
       "997    [input_ids, attention_mask]\n",
       "998    [input_ids, attention_mask]\n",
       "999    [input_ids, attention_mask]\n",
       "Length: 1000, dtype: object"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenised_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clsoer look at two examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Two examples of tokens from tokenised_data:\n",
      "\n",
      "Preprocessed data:\n",
      "4.115,4.567;2.628,4.406;1.746,3.903;1.283,3.281;1.064,2.682;0.985,2.167;1.001,1.747;1.097,1.415;1.276,1.159;1.55,0.967;1.943,0.829;2.482,0.737;3.2,0.687;4.123,0.678;5.257,0.716;6.562,0.815;7.924,0.998;9.13,1.307;9.86,1.792;9.749,2.494;8.564,3.374;6.523,4.207;4.348,4.651;2.719,4.538;1.766,4.031;1.271,3.387;1.035,2.764;0.945,2.226;0.951,1.787;1.036,1.44;1.201,1.173;1.456,0.973;1.825,0.827;2.337,0.728;3.024,0.67;3.917,0.652;5.029,0.68;6.335,0.763;7.739,0.925;9.045,1.203;9.947,1.649;10.062,2.316;9.091,3.192;7.12,4.099;4.814,4.678;2.985,4.677;1.89,4.211;1.314,3.563;1.034,2.913;0.918,2.344;0.904,1.876;0.969,1.505;1.11,1.218;1.338,1.001;1.671,0.841;2.137,0.73;2.772,0.661;3.609,0.631;4.669,0.644;5.946,0.706;7.369,0.838;8.777,1.073;9.892,1.46;10.333,2.058;9.727,2.895;7.978,3.859;5.6,4.616;3.489,4.82;2.134,4.464;1.414,3.824;1.061,3.142;0.904,2.528;0.861,2.019;0.9,1.612;1.013,1.295;1.205,1.054;1.493,0.875;1.904,0.747;2.469,0.663;3.226,0.618;4.206,0.612;5.418,0.651;6.824,0.75;8.304,0.935;9.625,1.25;10.438,1.755;10.32,2.508;8.998,3.473;6.709,4.402;4.307,4.883;2.589,4.729;1.63,4.156;1.149,3.455;0.924,2.793;0.84,2.229;0.846,1.774;0.925,1.417;1.078,1.143;1.317,0.937;1.665,0.787\n",
      "\n",
      "After tokenisation:\n",
      "[19, 13, 16, 16, 20, 11, 19, 13, 20, 21, 22, 26, 17, 13, 21, 17, 23, 11, 19, 13, 19, 15, 21, 26, 16, 13, 22, 19, 21, 11, 18, 13, 24, 15, 18, 26, 16, 13, 17, 23, 18, 11, 18, 13, 17, 23, 16, 26, 16, 13, 15, 21, 19, 11, 17, 13, 21, 23, 17, 26, 15, 13, 24, 23, 20, 11, 17, 13, 16, 21, 22, 26, 16, 13, 15, 15, 16, 11, 16, 13, 22, 19, 22, 26, 16, 13, 15, 24, 22, 11, 16, 13, 19, 16, 20, 26, 16, 13, 17, 22, 21, 11, 16, 13, 16, 20, 24, 26, 16, 13, 20, 20, 11, 15, 13, 24, 21, 22, 26, 16, 13, 24, 19, 18, 11, 15, 13, 23, 17, 24, 26, 17, 13, 19, 23, 17, 11, 15, 13, 22, 18, 22, 26, 18, 13, 17, 11, 15, 13, 21, 23, 22, 26, 19, 13, 16, 17, 18, 11, 15, 13, 21, 22, 23, 26, 20, 13, 17, 20, 22, 11, 15, 13, 22, 16, 21, 26, 21, 13, 20, 21, 17, 11, 15, 13, 23, 16, 20, 26, 22, 13, 24, 17, 19, 11, 15, 13, 24, 24, 23, 26, 24, 13, 16, 18, 11, 16, 13, 18, 15, 22, 26, 24, 13, 23, 21, 11, 16, 13, 22, 24, 17, 26, 24, 13, 22, 19, 24, 11, 17, 13, 19, 24, 19, 26, 23, 13, 20, 21, 19, 11, 18, 13, 18, 22, 19, 26, 21, 13, 20, 17, 18, 11, 19, 13, 17, 15, 22, 26, 19, 13, 18, 19, 23, 11, 19, 13, 21, 20, 16, 26, 17, 13, 22, 16, 24, 11, 19, 13, 20, 18, 23, 26, 16, 13, 22, 21, 21, 11, 19, 13, 15, 18, 16, 26, 16, 13, 17, 22, 16, 11, 18, 13, 18, 23, 22, 26, 16, 13, 15, 18, 20, 11, 17, 13, 22, 21, 19, 26, 15, 13, 24, 19, 20, 11, 17, 13, 17, 17, 21, 26, 15, 13, 24, 20, 16, 11, 16, 13, 22, 23, 22, 26, 16, 13, 15, 18, 21, 11, 16, 13, 19, 19, 26, 16, 13, 17, 15, 16, 11, 16, 13, 16, 22, 18, 26, 16, 13, 19, 20, 21, 11, 15, 13, 24, 22, 18, 26, 16, 13, 23, 17, 20, 11, 15, 13, 23, 17, 22, 26, 17, 13, 18, 18, 22, 11, 15, 13, 22, 17, 23, 26, 18, 13, 15, 17, 19, 11, 15, 13, 21, 22, 26, 18, 13, 24, 16, 22, 11, 15, 13, 21, 20, 17, 26, 20, 13, 15, 17, 24, 11, 15, 13, 21, 23, 26, 21, 13, 18, 18, 20, 11, 15, 13, 22, 21, 18, 26, 22, 13, 22, 18, 24, 11, 15, 13, 24, 17, 20, 26, 24, 13, 15, 19, 20, 11, 16, 13, 17, 15, 18, 26, 24, 13, 24, 19, 22, 11, 16, 13, 21, 19, 24, 26, 16, 15, 13, 15, 21, 17, 11, 17, 13, 18, 16, 21, 26, 24, 13, 15, 24, 16, 11, 18, 13, 16, 24, 17, 26, 22, 13, 16, 17, 11, 19, 13, 15, 24, 24, 26, 19, 13, 23, 16, 19, 11, 19, 13, 21, 22, 23, 26, 17, 13, 24, 23, 20, 11, 19, 13, 21, 22, 22, 26, 16, 13, 23, 24, 11, 19, 13, 17, 16, 16, 26, 16, 13, 18, 16, 19, 11, 18, 13, 20, 21, 18, 26, 16, 13, 15, 18, 19, 11, 17, 13, 24, 16, 18, 26, 15, 13, 24, 16, 23, 11, 17, 13, 18, 19, 19, 26, 15, 13, 24, 15, 19, 11, 16, 13, 23, 22, 21, 26, 15, 13, 24, 21, 24, 11, 16, 13, 20, 15, 20, 26, 16, 13, 16, 16, 11, 16, 13, 17, 16, 23, 26, 16, 13, 18, 18, 23, 11, 16, 13, 15, 15, 16, 26, 16, 13, 21, 22, 16, 11, 15, 13, 23, 19, 16, 26, 17, 13, 16, 18, 22, 11, 15, 13, 22, 18, 26, 17, 13, 22, 22, 17, 11, 15, 13, 21, 21, 16, 26, 18, 13, 21, 15, 24, 11, 15, 13, 21, 18, 16, 26, 19, 13, 21, 21, 24, 11, 15, 13, 21, 19, 19, 26, 20, 13, 24, 19, 21, 11, 15, 13, 22, 15, 21, 26, 22, 13, 18, 21, 24, 11, 15, 13, 23, 18, 23, 26, 23, 13, 22, 22, 22, 11, 16, 13, 15, 22, 18, 26, 24, 13, 23, 24, 17, 11, 16, 13, 19, 21, 26, 16, 15, 13, 18, 18, 18, 11, 17, 13, 15, 20, 23, 26, 24, 13, 22, 17, 22, 11, 17, 13, 23, 24, 20, 26, 22, 13, 24, 22, 23, 11, 18, 13, 23, 20, 24, 26, 20, 13, 21, 11, 19, 13, 21, 16, 21, 26, 18, 13, 19, 23, 24, 11, 19, 13, 23, 17, 26, 17, 13, 16, 18, 19, 11, 19, 13, 19, 21, 19, 26, 16, 13, 19, 16, 19, 11, 18, 13, 23, 17, 19, 26, 16, 13, 15, 21, 16, 11, 18, 13, 16, 19, 17, 26, 15, 13, 24, 15, 19, 11, 17, 13, 20, 17, 23, 26, 15, 13, 23, 21, 16, 11, 17, 13, 15, 16, 24, 26, 15, 13, 24, 11, 16, 13, 21, 16, 17, 26, 16, 13, 15, 16, 18, 11, 16, 13, 17, 24, 20, 26, 16, 13, 17, 15, 20, 11, 16, 13, 15, 20, 19, 26, 16, 13, 19, 24, 18, 11, 15, 13, 23, 22, 20, 26, 16, 13, 24, 15, 19, 11, 15, 13, 22, 19, 22, 26, 17, 13, 19, 21, 24, 11, 15, 13, 21, 21, 18, 26, 18, 13, 17, 17, 21, 11, 15, 13, 21, 16, 23, 26, 19, 13, 17, 15, 21, 11, 15, 13, 21, 16, 17, 26, 20, 13, 19, 16, 23, 11, 15, 13, 21, 20, 16, 26, 21, 13, 23, 17, 19, 11, 15, 13, 22, 20, 26, 23, 13, 18, 15, 19, 11, 15, 13, 24, 18, 20, 26, 24, 13, 21, 17, 20, 11, 16, 13, 17, 20, 26, 16, 15, 13, 19, 18, 23, 11, 16, 13, 22, 20, 20, 26, 16, 15, 13, 18, 17, 11, 17, 13, 20, 15, 23, 26, 23, 13, 24, 24, 23, 11, 18, 13, 19, 22, 18, 26, 21, 13, 22, 15, 24, 11, 19, 13, 19, 15, 17, 26, 19, 13, 18, 15, 22, 11, 19, 13, 23, 23, 18, 26, 17, 13, 20, 23, 24, 11, 19, 13, 22, 17, 24, 26, 16, 13, 21, 18, 11, 19, 13, 16, 20, 21, 26, 16, 13, 16, 19, 24, 11, 18, 13, 19, 20, 20, 26, 15, 13, 24, 17, 19, 11, 17, 13, 22, 24, 18, 26, 15, 13, 23, 19, 11, 17, 13, 17, 17, 24, 26, 15, 13, 23, 19, 21, 11, 16, 13, 22, 22, 19, 26, 15, 13, 24, 17, 20, 11, 16, 13, 19, 16, 22, 26, 16, 13, 15, 22, 23, 11, 16, 13, 16, 19, 18, 26, 16, 13, 18, 16, 22, 11, 15, 13, 24, 18, 22, 26, 16, 13, 21, 21, 20, 11, 15, 13, 22, 23, 22]\n",
      "Length of the above token: 1180\n",
      "\n",
      "\n",
      "Preprocessed data:\n",
      "4.738,4.035;3.557,3.817;2.752,3.472;2.234,3.067;1.901,2.664;1.698,2.287;1.586,1.95;1.539,1.657;1.545,1.407;1.596,1.196;1.687,1.02;1.817,0.875;1.988,0.755;2.201,0.657;2.458,0.577;2.764,0.514;3.12,0.463;3.53,0.425;3.995,0.396;4.516,0.377;5.09,0.366;5.713,0.364;6.376,0.37;7.068,0.385;7.771,0.411;8.466,0.449;9.128,0.501;9.73,0.57;10.243,0.66;10.634,0.775;10.872,0.917;10.93,1.091;10.787,1.296;10.434,1.529;9.878,1.779;9.149,2.029;8.295,2.259;7.384,2.445;6.484,2.566;5.65,2.615;4.929,2.588;4.336,2.497;3.871,2.361;3.522,2.195;3.274,2.017;3.111,1.838;3.021,1.666;2.992,1.506;3.018,1.362;3.093,1.234;3.213,1.123;3.376,1.027;3.579,0.947;3.822,0.882;4.101,0.829;4.415,0.789;4.761,0.76;5.134,0.742;5.528,0.736;5.935,0.74;6.346,0.755;6.75,0.782;7.134,0.821;7.482,0.873;7.782,0.938;8.016,1.016;8.173,1.108;8.241,1.213;8.212,1.329;8.086,1.452;7.865,1.578;7.561,1.7;7.19,1.812;6.775,1.905;6.338,1.974;5.904,2.014;5.493,2.023;5.121,2.004;4.801,1.958;4.535,1.892;4.326,1.811;4.171,1.721;4.068,1.628;4.012,1.535;3.999,1.445;4.026,1.362;4.09,1.285;4.187,1.217;4.316,1.158;4.473,1.107;4.654,1.066;4.858,1.034;5.079,1.011;5.314,0.997;5.556,0.992;5.8,0.996;6.04,1.008;6.268,1.03;6.477,1.06;6.659,1.098\n",
      "\n",
      "After tokenisation:\n",
      "[19, 13, 22, 18, 23, 11, 19, 13, 15, 18, 20, 26, 18, 13, 20, 20, 22, 11, 18, 13, 23, 16, 22, 26, 17, 13, 22, 20, 17, 11, 18, 13, 19, 22, 17, 26, 17, 13, 17, 18, 19, 11, 18, 13, 15, 21, 22, 26, 16, 13, 24, 15, 16, 11, 17, 13, 21, 21, 19, 26, 16, 13, 21, 24, 23, 11, 17, 13, 17, 23, 22, 26, 16, 13, 20, 23, 21, 11, 16, 13, 24, 20, 26, 16, 13, 20, 18, 24, 11, 16, 13, 21, 20, 22, 26, 16, 13, 20, 19, 20, 11, 16, 13, 19, 15, 22, 26, 16, 13, 20, 24, 21, 11, 16, 13, 16, 24, 21, 26, 16, 13, 21, 23, 22, 11, 16, 13, 15, 17, 26, 16, 13, 23, 16, 22, 11, 15, 13, 23, 22, 20, 26, 16, 13, 24, 23, 23, 11, 15, 13, 22, 20, 20, 26, 17, 13, 17, 15, 16, 11, 15, 13, 21, 20, 22, 26, 17, 13, 19, 20, 23, 11, 15, 13, 20, 22, 22, 26, 17, 13, 22, 21, 19, 11, 15, 13, 20, 16, 19, 26, 18, 13, 16, 17, 11, 15, 13, 19, 21, 18, 26, 18, 13, 20, 18, 11, 15, 13, 19, 17, 20, 26, 18, 13, 24, 24, 20, 11, 15, 13, 18, 24, 21, 26, 19, 13, 20, 16, 21, 11, 15, 13, 18, 22, 22, 26, 20, 13, 15, 24, 11, 15, 13, 18, 21, 21, 26, 20, 13, 22, 16, 18, 11, 15, 13, 18, 21, 19, 26, 21, 13, 18, 22, 21, 11, 15, 13, 18, 22, 26, 22, 13, 15, 21, 23, 11, 15, 13, 18, 23, 20, 26, 22, 13, 22, 22, 16, 11, 15, 13, 19, 16, 16, 26, 23, 13, 19, 21, 21, 11, 15, 13, 19, 19, 24, 26, 24, 13, 16, 17, 23, 11, 15, 13, 20, 15, 16, 26, 24, 13, 22, 18, 11, 15, 13, 20, 22, 26, 16, 15, 13, 17, 19, 18, 11, 15, 13, 21, 21, 26, 16, 15, 13, 21, 18, 19, 11, 15, 13, 22, 22, 20, 26, 16, 15, 13, 23, 22, 17, 11, 15, 13, 24, 16, 22, 26, 16, 15, 13, 24, 18, 11, 16, 13, 15, 24, 16, 26, 16, 15, 13, 22, 23, 22, 11, 16, 13, 17, 24, 21, 26, 16, 15, 13, 19, 18, 19, 11, 16, 13, 20, 17, 24, 26, 24, 13, 23, 22, 23, 11, 16, 13, 22, 22, 24, 26, 24, 13, 16, 19, 24, 11, 17, 13, 15, 17, 24, 26, 23, 13, 17, 24, 20, 11, 17, 13, 17, 20, 24, 26, 22, 13, 18, 23, 19, 11, 17, 13, 19, 19, 20, 26, 21, 13, 19, 23, 19, 11, 17, 13, 20, 21, 21, 26, 20, 13, 21, 20, 11, 17, 13, 21, 16, 20, 26, 19, 13, 24, 17, 24, 11, 17, 13, 20, 23, 23, 26, 19, 13, 18, 18, 21, 11, 17, 13, 19, 24, 22, 26, 18, 13, 23, 22, 16, 11, 17, 13, 18, 21, 16, 26, 18, 13, 20, 17, 17, 11, 17, 13, 16, 24, 20, 26, 18, 13, 17, 22, 19, 11, 17, 13, 15, 16, 22, 26, 18, 13, 16, 16, 16, 11, 16, 13, 23, 18, 23, 26, 18, 13, 15, 17, 16, 11, 16, 13, 21, 21, 21, 26, 17, 13, 24, 24, 17, 11, 16, 13, 20, 15, 21, 26, 18, 13, 15, 16, 23, 11, 16, 13, 18, 21, 17, 26, 18, 13, 15, 24, 18, 11, 16, 13, 17, 18, 19, 26, 18, 13, 17, 16, 18, 11, 16, 13, 16, 17, 18, 26, 18, 13, 18, 22, 21, 11, 16, 13, 15, 17, 22, 26, 18, 13, 20, 22, 24, 11, 15, 13, 24, 19, 22, 26, 18, 13, 23, 17, 17, 11, 15, 13, 23, 23, 17, 26, 19, 13, 16, 15, 16, 11, 15, 13, 23, 17, 24, 26, 19, 13, 19, 16, 20, 11, 15, 13, 22, 23, 24, 26, 19, 13, 22, 21, 16, 11, 15, 13, 22, 21, 26, 20, 13, 16, 18, 19, 11, 15, 13, 22, 19, 17, 26, 20, 13, 20, 17, 23, 11, 15, 13, 22, 18, 21, 26, 20, 13, 24, 18, 20, 11, 15, 13, 22, 19, 26, 21, 13, 18, 19, 21, 11, 15, 13, 22, 20, 20, 26, 21, 13, 22, 20, 11, 15, 13, 22, 23, 17, 26, 22, 13, 16, 18, 19, 11, 15, 13, 23, 17, 16, 26, 22, 13, 19, 23, 17, 11, 15, 13, 23, 22, 18, 26, 22, 13, 22, 23, 17, 11, 15, 13, 24, 18, 23, 26, 23, 13, 15, 16, 21, 11, 16, 13, 15, 16, 21, 26, 23, 13, 16, 22, 18, 11, 16, 13, 16, 15, 23, 26, 23, 13, 17, 19, 16, 11, 16, 13, 17, 16, 18, 26, 23, 13, 17, 16, 17, 11, 16, 13, 18, 17, 24, 26, 23, 13, 15, 23, 21, 11, 16, 13, 19, 20, 17, 26, 22, 13, 23, 21, 20, 11, 16, 13, 20, 22, 23, 26, 22, 13, 20, 21, 16, 11, 16, 13, 22, 26, 22, 13, 16, 24, 11, 16, 13, 23, 16, 17, 26, 21, 13, 22, 22, 20, 11, 16, 13, 24, 15, 20, 26, 21, 13, 18, 18, 23, 11, 16, 13, 24, 22, 19, 26, 20, 13, 24, 15, 19, 11, 17, 13, 15, 16, 19, 26, 20, 13, 19, 24, 18, 11, 17, 13, 15, 17, 18, 26, 20, 13, 16, 17, 16, 11, 17, 13, 15, 15, 19, 26, 19, 13, 23, 15, 16, 11, 16, 13, 24, 20, 23, 26, 19, 13, 20, 18, 20, 11, 16, 13, 23, 24, 17, 26, 19, 13, 18, 17, 21, 11, 16, 13, 23, 16, 16, 26, 19, 13, 16, 22, 16, 11, 16, 13, 22, 17, 16, 26, 19, 13, 15, 21, 23, 11, 16, 13, 21, 17, 23, 26, 19, 13, 15, 16, 17, 11, 16, 13, 20, 18, 20, 26, 18, 13, 24, 24, 24, 11, 16, 13, 19, 19, 20, 26, 19, 13, 15, 17, 21, 11, 16, 13, 18, 21, 17, 26, 19, 13, 15, 24, 11, 16, 13, 17, 23, 20, 26, 19, 13, 16, 23, 22, 11, 16, 13, 17, 16, 22, 26, 19, 13, 18, 16, 21, 11, 16, 13, 16, 20, 23, 26, 19, 13, 19, 22, 18, 11, 16, 13, 16, 15, 22, 26, 19, 13, 21, 20, 19, 11, 16, 13, 15, 21, 21, 26, 19, 13, 23, 20, 23, 11, 16, 13, 15, 18, 19, 26, 20, 13, 15, 22, 24, 11, 16, 13, 15, 16, 16, 26, 20, 13, 18, 16, 19, 11, 15, 13, 24, 24, 22, 26, 20, 13, 20, 20, 21, 11, 15, 13, 24, 24, 17, 26, 20, 13, 23, 11, 15, 13, 24, 24, 21, 26, 21, 13, 15, 19, 11, 16, 13, 15, 15, 23, 26, 21, 13, 17, 21, 23, 11, 16, 13, 15, 18, 26, 21, 13, 19, 22, 22, 11, 16, 13, 15, 21, 26, 21, 13, 21, 20, 24, 11, 16, 13, 15, 24, 23]\n",
      "Length of the above token: 1182\n"
     ]
    }
   ],
   "source": [
    "# Print tokenised output for the first system\n",
    "print('Two examples of tokens from tokenised_data:')\n",
    "print('')\n",
    "print('Preprocessed data:')\n",
    "print(traject_scaled_string[3])\n",
    "print('')\n",
    "print('After tokenisation:')\n",
    "print(tokenised_data.iloc[3][\"input_ids\"].squeeze().tolist())  # Tokenised tensor\n",
    "print('Length of the above token:',len(tokenised_data.iloc[3][\"input_ids\"].squeeze().tolist()))  # Tokenised tensor\n",
    "print('')\n",
    "print('')\n",
    "print('Preprocessed data:')\n",
    "print(traject_scaled_string[990])\n",
    "print('')\n",
    "print('After tokenisation:')\n",
    "print(tokenised_data.iloc[990][\"input_ids\"].squeeze().tolist())\n",
    "print('Length of the above token:',len(tokenised_data.iloc[990][\"input_ids\"].squeeze().tolist()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary of <b>Part 2 (a)</b>:\n",
    "\n",
    "- Load the predator-prey dataset from the `lotka_volterra_data.h5` file.\n",
    "- Applied the LLMTIME preprocessing scheme, this includes:\n",
    "    - Scale the numaric values using the fucntion ` scaling_operator` (in file `preprocessor.py`).\n",
    "    - Round the values to a fixed number of decimal places (we have used 3 d.p.).\n",
    "    - Converted the sequences into formatted strings using:\n",
    "        - Commas to separate variables at each timestep.\n",
    "        - Semicolons to separate different timesteps.\n",
    "        - Collection of every system.\n",
    "- Tokenised the processed sequences using the Qwen2.5 tokeniser.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 (b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "m2_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
