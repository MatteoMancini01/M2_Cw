{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3 (a) (Continued..)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plan of action:\n",
    "\n",
    "- Preprocess the data, using `load_and_preprocess`, and split the data into `train_texts`, `val_texts` and `val_text_70` $\\equiv$ 900, 100 and 100 (systems). \n",
    "- The two validation sets `val_texts` and `val_text_70` have the same `shape` but:\n",
    "    - In `val_texts` each system has the full 100 pairs of prey and predators\n",
    "    - In `val_texts_70` each system has only the first 70 pairs of prey and predators\n",
    "- We train the model on tokenised `train_texts`\n",
    "- We validated the model by predicting the remaining 30 pair points in each of the 100 system in tokenised `val_texts_70`. \n",
    "- We then compare the predicted results from `val_texts_70` to the gruond truth data `val_texts` (or `true_val_values` obtained with `data_scale_split`)\n",
    "- Just like for the untrained models we then want to compute MSE and RMSE \n",
    "- And report the loss/perplexity of each trained models\n",
    "\n",
    "We are recomended to train our model up to 10,000 steps, but we have a budgeted number of flops overall for training $10^{17}$ and due to computational power required, we are going to proceed with fewer steps first, also to familiarise with the traing procedure, before increasing the number of steps and using HPC.\n",
    "\n",
    "ESSENTIALLY\n",
    "\n",
    "“We trained on 900 systems, validated on 100 full sequences for loss monitoring, and evaluated forecasting performance by generating future predictions given the first 70 steps from each validation sequence.”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from src.preprocessor import*\n",
    "from src.plotting import*\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from src.minimal_lora_eval import process_sequences\n",
    "from src.qwen import tokenize_time_series_np\n",
    "from src.minimal_lora_eval import evaluate_loss, process_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# Set seed\n",
    "torch.manual_seed(23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.minimal_lora_eval import model, tokenizer # Import model and tokenizer from qwen with lora injected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts, val_texts, val_texts_70 = load_and_preprocess(\"data/lotka_volterra_data.h5\") # preprocess the data\n",
    "\n",
    "# split the data not converting to string, this will be used to compute the error in the validation set\n",
    "_, true_val_values, _ = data_scale_split(\"data/lotka_volterra_data.h5\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenise with overlapping chucks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Here we are:\n",
    "1. Tokenising each time series using Qwen tokenizer\n",
    "2. Chunking the tokenised output into sliding windows of tokens (for training)\n",
    "3. Pad shorter sequences to a fixed length\n",
    "\"\"\"\n",
    "\n",
    "train_input_ids = process_sequences(train_texts, tokenizer, max_length=512, stride=256)\n",
    "val_input_ids = process_sequences(val_texts, tokenizer, max_length=512, stride=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason why we stride our training dataset, is to generate more overlap during training, wich leads to more signal from each long sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorDataset allows you to wrap tensors into a dataset object\n",
    "# DataLoader provides minibatch iteration over the dataset\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Accelerator from Hugging Face simplifies GPU/multi-device/mixed-precision training\n",
    "from accelerate import Accelerator\n",
    "\n",
    "# tqdm is used for progress bars — makes training loops easier to monitor\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set hyperparameters\n",
    "batch_size = 4  # Number of sequences per training batch\n",
    "learning_rate = 1e-5 # Learning rate for the optimizer\n",
    "\n",
    "# Set up the optimizer to update only trainable parameters (e.g., LoRA adapters + LM head bias)\n",
    "optimizer = torch.optim.Adam(\n",
    "    (p for p in model.parameters() if p.requires_grad), lr=learning_rate\n",
    ")\n",
    "# Wrap the training data into a TensorDataset and DataLoader for batching\n",
    "train_dataset = TensorDataset(train_input_ids)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Initialize Hugging Face Accelerator for device/mixed-precision handling\n",
    "accelerator = Accelerator()\n",
    "\n",
    "# Prepare model, optimizer, and dataloader with Accelerator\n",
    "model, optimizer, train_loader = accelerator.prepare(model, optimizer, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Set model to training mode\n",
    "model.train()\n",
    "steps = 0\n",
    "\n",
    "max_steps = 10000\n",
    "# Start training loop\n",
    "while steps < max_steps:  # QPLPPP = 10,000 steps\n",
    "    for (batch,) in tqdm(train_loader, desc=f\"Steps {steps}\"):\n",
    "\n",
    "         # Reset gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass with labels = input_ids for language modeling (next-token prediction)\n",
    "        outputs = model(batch, labels=batch)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        # Backpropagation using Accelerator\n",
    "        accelerator.backward(loss)\n",
    "        optimizer.step()\n",
    "\n",
    "        # Step counter\n",
    "        steps += 1\n",
    "\n",
    "        # Break loop after desired number of steps\n",
    "        if steps >= max_steps:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing validation loss before prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collecting loss and perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"After training with {max_steps} steps\")\n",
    "print(f\"Training loss: {loss.item():.4f}\")\n",
    "perplexity = torch.exp(loss).item()\n",
    "print(f\"Perplexity: {perplexity:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking if weights were updated during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"{name}: {param.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving LoRA Weights only (Efficitent checkpoints).\n",
    "\n",
    "Save the LoRA adapter weights (not the full Qwen model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract LoRA-only weights\n",
    "lora_state_dict = {\n",
    "    name: param.cpu()\n",
    "    for name, param in model.named_parameters()\n",
    "    if param.requires_grad\n",
    "}\n",
    "\n",
    "torch.save(lora_state_dict, \"trained_lora_3a_10000/lora_weight_matrices_10000.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = TensorDataset(val_input_ids)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4)\n",
    "val_loader = accelerator.prepare(val_loader)\n",
    "\n",
    "val_loss = evaluate_loss(model, val_loader)\n",
    "perplexity_val = np.exp(val_loss)\n",
    "print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "print(f\"Validation Perplexity: {perplexity_val}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving loss and perplexity experienced in training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_perplexity_df = pd.DataFrame({\n",
    "    \"Loss\": [loss.item(), val_loss],  \n",
    "    \"Perplexity\": [perplexity, perplexity_val],  \n",
    "\n",
    "}, ['Training', 'Validation'])\n",
    "loss_perplexity_df.to_csv(\"trained_lora_3a_10000/Loss_Perplexity_Lora_10000\", index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_length_array(tokenized_string):\n",
    "\n",
    "    \n",
    "    max_lengths = np.array([entry[\"input_ids\"].shape[1] for entry in tokenized_string])\n",
    "    return max_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokeniseing data series\n",
    "tok_val_texts = tokenize_time_series_np(val_texts)\n",
    "tok_val_texts_70 = tokenize_time_series_np(val_texts_70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model is trained and validated we can perform prdictions using `model.generate()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unwrap the model from Accelerate (optional) and move to GPU\n",
    "model = accelerator.unwrap_model(model)\n",
    "model.to(\"cuda\")\n",
    "model.eval()\n",
    "\n",
    "predictions_encoded = []\n",
    "\n",
    "# Loop over each validation sample for generation\n",
    "for i in tqdm(range(len(tok_val_texts_70)), desc=\"Generating predictions\"):\n",
    "    \n",
    "    # Extract tokenized input and attention mask for system i\n",
    "    input_ids = tok_val_texts_70[i][\"input_ids\"].to(model.device)\n",
    "    attention_mask = tok_val_texts_70[i][\"attention_mask\"].to(model.device)\n",
    "\n",
    "    # Use model.generate() to predict the next tokens\n",
    "    output = model.generate(\n",
    "        input_ids=input_ids,                 # Input sequence (first 70 timesteps)\n",
    "        attention_mask=attention_mask,       # Mask to ignore padding tokens\n",
    "        min_new_tokens=350,                   # Force at least 30 new tokens to be generated\n",
    "        max_new_tokens=400,                   # Upper bound to avoid runaway generation\n",
    "        do_sample=False,                     # Use deterministic decoding (greedy search)\n",
    "        eos_token_id=tokenizer.eos_token_id  # Stop generation at EOS if it appears\n",
    "    )\n",
    "\n",
    "    # Append generated output to the list (will decode later)\n",
    "    predictions_encoded.append(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_decoded = []\n",
    "\n",
    "for i in range(len(tok_val_texts_70)):\n",
    "    decoded_output = string_to_array(tokenizer.decode(predictions_encoded[i][0], skip_special_tokens=True))\n",
    "    predictions_decoded.append(decoded_output)\n",
    "\n",
    "predicted_output = []\n",
    "true_values = []\n",
    "# Collecting the last 30 pair from each array\n",
    "for i in range(len(predictions_decoded)):\n",
    "    sub_output = predictions_decoded[i]\n",
    "    sub_output_min_30 = sub_output[70:100]\n",
    "    true_v = true_val_values[i]\n",
    "    true_v_min_30 = true_v[70:]\n",
    "    predicted_output.append(sub_output_min_30)\n",
    "    true_values.append(true_v_min_30)\n",
    "\n",
    "# Computing MSE\n",
    "MSE_values = []\n",
    "\n",
    "for i in range(len(true_values)):\n",
    "\n",
    "    # Truncate to match the shorter list\n",
    "    min_length = min(len(predicted_output[i]), len(true_values[i]))\n",
    "    pr_out = predicted_output[i][:min_length]\n",
    "    true_val = true_values[i][:min_length]\n",
    "\n",
    "    # Compute the MSE\n",
    "    mse_prey = mean_absolute_error(pr_out[:,0], true_val[:,0]) # Computing MSE for prey\n",
    "    mse_predator = mean_absolute_error(pr_out[:,1], true_val[:,1]) # Computing MSE for predator\n",
    "    MSE_values.append([mse_prey, mse_predator])\n",
    "\n",
    "# Computing RMSE\n",
    "RMSE_values = np.sqrt(MSE_values)\n",
    "\n",
    "df_MSE_values = pd.DataFrame({\n",
    "    \"system_id\": np.arange(len(MSE_values)),  \n",
    "    \"MSE for prey\": np.array(MSE_values)[:, 0],  # Flatten prey values\n",
    "    \"MSE for predator\": np.array(MSE_values)[:, 1]  # Flatten predator values\n",
    "})\n",
    "df_RMSE_values = pd.DataFrame({\n",
    "    \"system_id\": np.arange(len(MSE_values)),  \n",
    "    \"RMSE for prey\": RMSE_values[:, 0],  # Flatten prey values\n",
    "    \"RMSE for predator\": RMSE_values[:, 1]  # Flatten predator values\n",
    "})\n",
    "\n",
    "# Computing the error for each system\n",
    "\n",
    "error_per_system = []\n",
    "\n",
    "for i in range(len(true_values)):\n",
    "\n",
    "    # Truncate to match the shorter list\n",
    "    min_length = min(len(predicted_output[i]), len(true_values[i]))\n",
    "    pr_out = predicted_output[i][:min_length]\n",
    "    true_val = true_values[i][:min_length]\n",
    "\n",
    "    # Computing errors\n",
    "    prey_error = true_val[:,0] - pr_out[:,0]\n",
    "    predator_error = true_val[:,1] - pr_out[:,1]\n",
    "    \n",
    "    # Collecting errors into a list\n",
    "    error_per_system.append([prey_error, predator_error])\n",
    "\n",
    "print('Length of error_per_system:',len(error_per_system))\n",
    "\n",
    "from src.plotting import PlotProject\n",
    "PlotProject.plot_hist_MSE(df_MSE_values,bins=30) # Plotting MSE distribution\n",
    "PlotProject.plot_hist_RMSE(df_RMSE_values,bins=30) # Plotting RMSE distribution\n",
    "PlotProject.plot_error_hist_system(error_per_system, 0, bins = 30)\n",
    "pred_vs_true_visualisation(predictions_decoded, true_val_values, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(\"trained_lora_3a_10000/predictions_decoded_trained_lora_3a_10000.npz\", *predictions_decoded)\n",
    "MSE_loaded = np.save(\"trained_lora_3a_10000/MSE_values_3a_10000.npy\", np.array(MSE_values))\n",
    "np.save('trained_lora_3a_10000/RMSE_values_3a_10000', RMSE_values)\n",
    "np.savez(\"trained_lora_part_3a/error_per_system_5000.npz\", *error_per_system)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reload LoRA weight matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#weight_matrices_10000 = model.load_state_dict(torch.load(\"trained_lora_3a_10000/lora_weight_matrices_10000.pt\"), strict=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(15,20): \n",
    "    pred_vs_true_visualisation(predictions_decoded, true_val_values, i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "m2_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
